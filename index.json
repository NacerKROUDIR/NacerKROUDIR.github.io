[{"content":"","date":null,"permalink":"/tags/blogs/","section":"Tags","summary":"","title":"blogs"},{"content":" Projects, Articles, Everything that I worked on will be here!\nFor more blogs, you can check out my published blogs on medium.\n","date":null,"permalink":"/blogs/","section":"Blogs","summary":"Projects, Articles, Everything that I worked on will be here!","title":"Blogs"},{"content":" Data science and artificial intelligence fresh graduate from ENSIMAG with a keen interest in turning raw data into meaningful stories. Proven track record of working with data analytics tools, such as Python, Microsoft Excel, Tableau. Passionate about generative AI and LLMs. With ability to complete tasks on time in both individual and team settings. Dependable and reliable, ready to learn and grow in this rapidly evolving field!\nEducation: # Master of Science in Informatics at Grenoble - Data Science and Artificial Intelligence # Ensimag - Grenoble INP # September 2023 - July 2024\nRelevant coursework: From Basic Machine Learning models to Advanced Kernel Learning, Advanced Machine Learning, Fundamentals of Data Processing and Distributed Knowledge, Cloud Computing, Large Scale Data Management.\nproject:\nData Analysis for Google Cluster Dataset: Conducted a detailed analysis with big data using Spark and built a report using insights and visualisations to communicate the key findings.\nMaster in Computer Engineering # Institute of Electronics and Electrical Engineering (IEEE Ex: INELEC), University of Boumerdes # September 2021 - July 2023\nRelevant coursework: Data Structures and Algorithms, Probability and Statistics, Database Management System, Computer Networking\nproject:\nEpidemic Simulation Framework: Epidemic Simulation Framework is an open-source project that aims at providing an interactive and educational tool for simulating the spread of a virus in a 2D plane. The project utilizes various parameters such as transmission rate, infection period, and recovery rate to generate different scenarios of an epidemic. The simulation tool is designed to visualize the epidemic in real-time, enabling users to observe the spread of the virus and the impact of various interventions such as quarantine, social distancing, and vaccination. The project aims at educating users about the importance of public health measures and the impact of individual behavior on the spread of an epidemic.\nData Analyst Nanodegree # udacity # November 2022 - February 2023\nA program delivered by Udacity and ALX. With various projects that are focused on different aspects of data analysis, including data wrangling, data visualization, and exploratory data analysis.\nBachelor in Electronics and Electrical Engineering # Institute of Electronics and Electrical Engineering (IEEE Ex: INELEC), University of Boumerdes # September 2018 / July 2021\nRelevant coursework: Linear Algebra, Calculus, Programming with C, Electrical Engineering, Digital Systems, Active Devices, Computer Architecture\nproject:\nQR Code based Presence Verification System: QR Code based Presence Verification System was developed to target a problem in the presence verification process used by Inelectronics Students Club. It optimises the procedure by generating and tracking participant‚Äôs QR codes for events and workshops, effectively managing attendance and reducing errors. It was utilized by the Club for World Engineering Day and several other workshops and events.\nIoT Based Security Camera: A raspberry pi based system that detects faces and track them. Also, it runs face recognition and takes a picture of unrecognized people and sends it via email.\nWork Experience: # Data Scientist Intern # DATATEGY # February 2024 - Present\nContributed to customer-oriented projects: translation project for RENAULT using GenAI / forecast of groundwater levels in France for summer 2024. Developed an open-source library called FairMango and integrated it with a SaaS platform to create fair models, ensuring compliance with the EU AI Act. A major advantage for the platform over competitors such as Dataiku. Data Scientist Intern # Yassir # May 2022 - September 2022\nDeveloped an artificial intelligence system to predict travel times between two points, reducing costs of using the GoogleMaps API for Yassir. Simulated and evaluated the model\u0026rsquo;s performance over different areas, resulting in a hybrid solution that leveraged the AI model in data-rich areas and the GoogleMaps API in areas with limited data. Software Engineer Intern # Brandt # February 2021 - July 2021\nBuilt an IoT based security camera using Raspberry-pi. Implemented face detection and tracking on two axis of motion using servo motors. Devoloped face recognition and an alarm system that saves the identity of any intruder. Skills # Skill Level Data Analysis ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Data Visualization ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Machine Learning ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Databases ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Business Intelligence ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Python ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è SQL ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Spark ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Git ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Google Cloud ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Big Query ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Microsoft Excel ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Microsoft PowerPoint ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Tableau ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Figma ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Organizations: # Co-manager of Communication Department # School of AI Algiers # September 2021 - August 2022\nOrganizer and Active Member in the Developement Department # Inelectronics Student Club # September 2019 - September 2021\nCertificates # Certificate Date Data Scientist with Python - DataCamp April 28, 2022 Fundamentals of Deep Learning - Nvidia December 26, 2021 Databases and SQL for Data Science - IBM December 13,2021 Machine Learning - Coursera September 24, 2021 Injaz El-Djazair - The Entrepreneur\u0026rsquo;s Journey 2019/2020 Languages # Language Level English ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Arabic ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è French ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ","date":"30 July 2024","permalink":"/resume/","section":"Nacer KROUDIR","summary":"Data science and artificial intelligence fresh graduate from ENSIMAG with a keen interest in turning raw data into meaningful stories.","title":"Hi, I'm Nacer KROUDIR üëã"},{"content":"Welcome to the exciting world of accessible large language models (LLMs)! With the advent of tools like Ollama, deploying and running powerful LLMs locally has never been easier.¬†In this blog, we will go through the steps to get started with Ollama, including installing the software and running the latest LLaMA 3.1 model. We\u0026rsquo;ll then take it a step further by deploying our LLM in a Docker container and using Open WebUI for an interactive web-based interface. This setup not only enhances the functionality and performance of the LLM but also provides a user-friendly environment for seamless interactions.\nollama Download Ollama software and the¬†LLM # Ollama is an open-source project designed to simplify the process of running large language models (LLMs) locally. It supports various models, including LLaMA 3, Phi 3, Mistral, and Gemma 2, and is available for macOS, Linux, and Windows. Ollama\u0026rsquo;s key features include compatibility with the OpenAI Chat Completions API, which allows users to leverage existing OpenAI tooling with locally running models, and support for tool calling, enabling models to perform more complex tasks by interacting with external tools‚Äã [1]. Once we have installed Ollama, we are just a step away from interacting with any powerful supported LLM. We will prompted to run this command on the terminal:\nollama run llama3 But we are going to try the latest version released as of (July 30, 2024) which is LLaMA 3.1. The new command is simply:\nollama run llama3.1 This command will download the LLaMA 3.1 model and initialize it. In no time, we will have a fully operational LLaMA 3.1 model right in our terminal, ready for us to interact with. By default, the 8 billion parameter model is downloaded, which is perfect for most applications. However, if you have the computational power and need more advanced capabilities, you can opt for the larger 70 billion or even the 405 billion parameter models.\nTerminal for interaction with LLaMA 3.1 But that\u0026rsquo;s just the beginning!\nElevate Your Experience with Open¬†WebUI # For a more robust and enjoyable experience, we can deploy our LLM in a Docker container and use the Open WebUI for an interactive web-based interface. Docker provides a powerful and consistent environment for running applications, ensuring that our models run smoothly and efficiently across different systems. It simplifies the setup process by encapsulating all the dependencies and configurations within a container, making it easier to deploy and manage the LLM. Open WebUI, on the other hand, offers a seamless, user-friendly interface akin to ChatGPT. This web-based interface enhances our interaction with the LLM by providing a clean and intuitive chat environment. Features like full Markdown and LaTeX support allow us to format responses richly, making the interaction more engaging and informative. Additionally, Open WebUI supports multi-modal interactions, enabling us to integrate text, pdf files and audio inputs seamlessly. With docker open and having the LLM downloaded locally, we use the following command to deploy Open WebUI with Ollama support on a docker container:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main After executing the command, a new container is created and can be observed in the Docker interface. We can now access our WebUI by navigating to http://localhost:3000 in our browser. This link directs us to the fully functional and interactive web-based interface, allowing us to start utilizing our AI model immediately and even offline! We start by selecting the model on the top left\nSelect a model‚Ää-‚ÄäWebUI interface And Enjoy the chat! You can upload PDF files, and the model will be capable of answering questions based on the content of those files, providing you with a highly interactive and informative experience. A brief demonstration of the LLaMa 3.1 (8B version) capabilities can be seen in the following:\nDemo 1‚Ää-‚ÄäWebUI interface Demo 2‚Ää-‚ÄäWebUI interface Reflections on Open-Source LLMs # Open-source large language models (LLMs) are transforming the landscape of artificial intelligence and machine learning. These models, which include popular names like LLaMA 3, Mistral, and others supported by projects such as Ollama, offer a range of advantages that are fueling innovation and democratizing access to advanced AI technology. Let\u0026rsquo;s dive into a discussion about the key benefits of open-source LLMs and how they are shaping the future of AI.\nAccessibility and Democratization: Open-source LLMs make cutting-edge technology accessible to a broader audience. Researchers, developers, and enthusiasts can experiment with and build upon these models without the need for expensive licenses or proprietary software. This democratization fosters a more inclusive AI community where innovation can thrive at all levels. Transparency and Trust: Transparency is a fundamental principle of open-source projects. It allows users to inspect the code, understand the model architectures, and verify the training processes, thereby building trust. This openness ensures that there are no hidden biases or undesirable behaviors embedded within the models. However, it\u0026rsquo;s important to note that the training dataset for some models, such as LLaMA 3.1, is not fully disclosed. The LLaMA 3.1 paper mentions that their dataset is created from various sources containing knowledge up to the end of 2023, but it does not provide detailed information about these sources. This lack of clarity raises questions about the ethical collection of the data‚Ää-‚Ääa challenge that all LLM development groups face. While transparency in code and architecture is crucial, transparency in data sources is equally important to fully address trust and ethical considerations in AI development. Collaborative Improvement: The open-source community thrives on collaboration. Contributions from diverse groups of developers lead to rapid advancements and improvements. Bugs are identified and fixed more quickly, and new features and optimizations are continuously added, enhancing the overall performance and utility of the models. Customization and Flexibility: Open-source LLMs offer unparalleled flexibility. Users can customize models to suit their specific needs, whether it\u0026rsquo;s fine-tuning them for particular applications, integrating them with other tools, or even modifying the core architecture. This level of customization is often not possible with proprietary models. Cost Efficiency: Open-source models eliminate licensing fees, making advanced AI more affordable. This is particularly beneficial for startups, educational institutions, and non-profit organizations that might not have the resources to invest in costly AI solutions. I quote from Mark Zuckerberg\u0026rsquo;s letter: \u0026ldquo;Developers can run inference on Llama 3.1 405B on their own infra at roughly 50% the cost of using closed models like GPT-4o, for both user-facing and offline inference tasks.\u0026rdquo; Given these advantages, it\u0026rsquo;s clear that open-source LLMs are not just a technical innovation but also a catalyst for broader societal benefits. They empower individuals and organizations to harness the power of AI in ways that were previously unimaginable. How do you think open-source LLMs will continue to shape the future of AI? What potential challenges and opportunities do you foresee in this rapidly evolving landscape?\n","date":null,"permalink":"/blogs/how-to-run-open-source-llms-with-a-chatgpt-like-web-experience/","section":"Blogs","summary":"Open-source large language models (LLMs) offer significant advantages, including enhanced accessibility, transparency, and customization. In this blog, we will explore how to install and run Ollama\u0026rsquo;s LLaMA 3.1 model locally, and deploy it using Docker and Open WebUI for a ChatGPT-like interactive experience.","title":"How to Run Open source LLMs with a ChatGPT-Like Web Experience"},{"content":"","date":null,"permalink":"/","section":"Nacer KROUDIR","summary":"","title":"Nacer KROUDIR"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Gala Groceries, a US-based tech-driven grocery chain, distinguishes itself through advanced technologies like IoT to gain a competitive advantage. Despite their commitment to offering top-notch, locally sourced fresh produce, maintaining consistent quality throughout the year poses challenges. Seeking Cognizant\u0026rsquo;s assistance, they aim to resolve a supply chain dilemma specific to perishable groceries, seeking to optimize stocking strategies and balance between excess storage costs and customer retention.\nObjectives # Take a deep look at the problem by exploring the sample data provided thoroughly. Come up with a strategic plan to solve the problem, outlining the steps we need to follow. Perform feature engineering and develop a predictive model capable of forecasting the storage levels of each product. Make a pipeline for the Data Engineers to train the model on the provided dataset and make predictions along with evaluation. Understanding the Problem # Stock management, also known as inventory management, is a critical aspect of supply chain management that involves planning, organizing, and controlling the acquisition, storage, distribution, and utilization of goods. It encompasses various strategies and techniques aimed at maintaining an optimal balance between supply and demand while minimizing costs and ensuring efficient operations. Effective stock management is essential for businesses to meet customer demands, reduce carrying costs, and improve overall supply chain performance.\nImportance of Stock Management in the Supply Chain:\nMeeting Customer Demand: Proper stock management ensures that products are available when and where customers want them. It prevents stockouts (running out of products) and overstock situations, allowing businesses to fulfill customer orders promptly and maintain high levels of customer satisfaction. Cost Control: Excess inventory ties up capital and incurs storage costs, while inadequate stock levels can lead to production disruptions and lost sales. Effective stock management helps strike a balance, reducing carrying costs, minimizing waste, and optimizing the use of resources. Supply Chain Efficiency: Efficient stock management contributes to the smooth functioning of the entire supply chain. By ensuring the right products are available at the right time, it minimizes disruptions and delays, leading to improved operational efficiency. Risk Mitigation: Fluctuations in demand, supply chain disruptions, and other external factors can impact a business\u0026rsquo;s ability to maintain steady stock levels. Effective stock management involves risk assessment and mitigation strategies to address potential disruptions and uncertainties. Working Capital Optimization: Excessive inventory ties up capital that could be invested in other areas of the business. Well-managed stock levels free up working capital, allowing companies to invest in growth opportunities or reduce debt. Supplier Relationships: Accurate stock management enables better communication with suppliers. Timely and accurate information about inventory levels helps in negotiating better terms, managing lead times, and building stronger relationships with suppliers. Stock Management Techniques:\nJust-In-Time (JIT): JIT is a strategy where inventory is ordered and received only as needed for production or customer orders. It reduces carrying costs but requires precise coordination and reliable suppliers. Safety Stock: This involves maintaining a certain level of extra inventory to buffer against unexpected demand fluctuations or supply chain disruptions. ABC Analysis: Items are categorized into A, B, and C categories based on their value and contribution to sales. A-items (high-value) are closely monitored, while C-items (low-value) may have less stringent control. Economic Order Quantity (EOQ): EOQ calculates the optimal order quantity that minimizes the total cost of ordering and holding inventory. It considers factors like carrying costs, ordering costs, and demand. Cross-Functional Collaboration: Effective stock management involves collaboration between various departments like sales, marketing, production, and logistics to align inventory levels with overall business goals. Demand Forecasting: Accurate demand forecasting helps predict future demand patterns, enabling businesses to adjust stock levels accordingly. Technology and Automation: Inventory management software, barcoding, RFID, and IoT technologies can provide real-time visibility into stock levels and streamline stock tracking and replenishment processes. In our case, we are combining IoT technologies that measures stock levels of different products with demand forecasting in order develop a model that can accurately predict the demand for each product and the best times to restock.\nData Exploration # After assessing the quality of the data. We start our data exploration by visualizing the distribution of number of transactions per hour.\nNumber of Transactions per Hour The number of transactions throughout the day are somewhat evenly distributed with 11am being the most active hour (738 transactions) and 3pm being the least active hour (669 transactions).\nDistribution of Product Categories There are a few key takeaways from the above barplot, we mention:\nFruits and Vegetables are the most sold categories making up 23.56% of the total transactions. This indicates a high demand for fresh products. packaged foods and baked goods also have a significant percentage of transactions, at 6.48% and 5.66% respectively. Canned foods, refrigerated items, and kitchen products have relatively lower percentages but still make up a noticeable portion of transactions. Meat, dairy, and beverages have similar percentages of around 4-5%, indicating that these categories are popular but are not purchased as much as fruits and vegetables. Cleaning products, baking supplies, snacks, and frozen items all have percentages in the range of 3-4%, indicating moderate demand for these categories. Seafood, medicine, baby products, condiments and sauces, personal care, pets, and spices and herbs have percentages below 3%, indicating a relatively lower demand for these categories. Distribution of Unit Price and Total Price The first plot indicates that the distribution of unit price is positively skewed towards the lower end, implying that the majority of sales involve products with lower prices rather than higher ones.\nThis observation aligns with expectations for a typical grocery store, where a larger number of inexpensive products are sold compared to a smaller number of high-priced items.\nSimilarly, the second plot reveals a similar trend for the total price, indicating a positive skewness in its distribution. This implies that a larger proportion of sales transactions involve lower total prices compared to higher total prices.\nJust as with unit prices, this observation is in line with the expectation that a grocery store would have a higher volume of transactions for lower-priced items, while fewer transactions would involve higher-priced items.\nAverage Unit Price and Total Price per Category From the above figure representing the average unit prices and total prices for each category, here are the key takeaways:\nThe unit prices and total prices vary across different categories, indicating differences in pricing for different types of products. Categories such as baby products, beverages, cleaning products, kitchen items, meat, medicine, and seafood have relatively higher unit prices and total prices. This suggests that these categories include higher-priced or premium products. Categories like fruit, snacks, spices and herbs, and vegetables have lower unit prices and total prices. This indicates that these categories consist of lower-priced or more affordable items. Categories like baked goods, dairy, frozen items, personal care, pets, and condiments and sauces have moderate unit prices and total prices, falling between the higher-priced and lower-priced categories. Distribution of Unit Price per Category The analysis of the unit price distributions reveals an interesting pattern for several categories, namely fruit, vegetables, spices and herbs, canned foods, packaged foods, condiments and sauces, seafood, baking, kitchen, and cleaning products. These categories exhibit bimodal distributions, with two distinct peaks observed at different ends of the price spectrum.\nThis bimodality suggests the presence of two distinct qualities or tiers within these product categories. On one end of the spectrum, there are products with lower unit prices, indicating a more affordable or budget-friendly option. On the other end, there are products with higher unit prices, reflecting a premium or higher-quality offering.\nStrategic Plan # Strategic Plan Feature Engineering # Since we have three datasets ‚Äì sales, stock level, and temperature ‚Äì we should combine them using the datetime column. However, prior to merging, it\u0026rsquo;s necessary to round the time values to the nearest hour. Merging datasets can create gaps due to missing values. Our solution involves judiciously applying suitable techniques to fill these gaps, ensuring a continuous and complete dataset. This process transforms missing values into valuable data points. While our dataset initially spans from 9am to 7pm, we recognize the need to encompass the entire day. By skillfully extending the dataset to cover all hours, we capture a broader context for analysis, thereby enriching the predictive capabilities. The dimension of time is unwrapped further, revealing three key temporal features: hour, day, and month. These features introduce depth to the dataset, contributing to a more nuanced and accurate predictive model. The path to accurate stock level prediction is illuminated by the Partial Autocorrelation Function (PACF) plot. Our analysis of this plot guides us to utilize lagged values of 1, 2, and 3, enabling the model to capture meaningful patterns and dependencies. Next we shifted other features -like quantity, temperature and total by one hour as it will be available at the time of making a prediction. customer_type and category were one hot encoded in order to be used for the model development. If you would like to get technical I highly recommend you checkout the full code implementation in this github repository Forecasting Model # First, we split the data by using the first 6 days for training and 1 last day for testing.\ntrain = data[data[\u0026#39;ds\u0026#39;]\u0026lt;\u0026#39;2022-03-07\u0026#39;] test = data[data[\u0026#39;ds\u0026#39;]\u0026gt;=\u0026#39;2022-03-07\u0026#39;] train.drop(\u0026#39;ds\u0026#39;, axis=1, inplace=True) test.drop(\u0026#39;ds\u0026#39;, axis=1, inplace=True) Next, we seperate our target variable from the features that will be used to train the model.\ny_train, y_test = train[\u0026#39;y\u0026#39;], test[\u0026#39;y\u0026#39;] X_train, X_test = train.drop(\u0026#39;y\u0026#39;, axis=1), test.drop(\u0026#39;y\u0026#39;, axis=1) The following function will be used to evaluate the model performance. The evaluation metrics chosen are:\nMean Squared Error: The average of the squared differences between predicted and actual values, used to measure the overall quality of a predictive model. Root Mean Squared Error: The square root of the average of the squared differences between predicted and actual values, providing a measure of the model\u0026rsquo;s prediction error in the same units as the original data. Mean Absolute Error: The average of the absolute differences between predicted and actual values, offering a straightforward measure of the model\u0026rsquo;s prediction accuracy. R Squared: A statistical measure indicating the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. def evaluate(model_, X_test_, y_test_): y_pred = model_.predict(X_test_) results = pd.DataFrame({\u0026#34;MSE\u0026#34; : [mean_squared_error(y_test_, y_pred)], \u0026#34;RMSE\u0026#34; : [np.sqrt(mean_squared_error(y_test_, y_pred))], \u0026#34;MAE\u0026#34; : [mean_absolute_error(y_test_, y_pred)], \u0026#34;R2\u0026#34; : [r2_score(y_test_, y_pred)]}) return results Several machine learning and deep learning algorithms were implemented - namely:\nXGBRegressor LGBMRegressor Long Short Term Memory GridSearchCV was used with XGBRegressor and LGBMRegressor for hyper parameter tuning.\nXGBRegressor # The XGBRegressor is a machine learning model belonging to the XGBoost (Extreme Gradient Boosting) framework: it is an ensemble learning technique designed for regression tasks, proficient at generating robust predictions. Within XGBoost, the XGBRegressor harnesses the collective power of multiple individual weak models to yield a potent predictive model. These weak models, in the context of XGBoost, are decision trees. The model employs a gradient boosting approach, wherein the algorithm iteratively constructs decision trees to minimize the discrepancies between predicted and actual target values.\nOnce the decision trees have been trained, XGBoost makes predictions by combining the predictions of all the trees using a weighted average. The weights for each tree are learned during training using the same objective function. This allows the algorithm to automatically learn which trees are more important and should be given more weight in the final prediction.\nparameters = {\u0026#39;n_estimators\u0026#39;:[128,200], \u0026#39;max_depth\u0026#39;:[7,9], \u0026#39;learning_rate\u0026#39;:[0.03,0.04]} model = xgb.XGBRegressor(n_jobs=-1, random_state=44) gs = GridSearchCV(model, parameters, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.03 max_depth 7 n_estimators 128 The evaluation of the model gives the following results:\nevaluate(gs, X_test, y_test) Metric Score Mean Squared Error 226.496116 Root Mean Squared Error 15.049788 Mean Absolute Error 7.431851 R Squared 72.723% LGBMRegressor # The LGBMRegressor, short for Light Gradient Boosting Machine Regressor, is a sophisticated machine learning model tailored for regression tasks. It falls under the domain of gradient boosting algorithms, specifically designed to provide rapid and highly accurate predictions. The distinguishing feature of the LGBMRegressor lies in its emphasis on optimization for efficiency and performance, making it particularly suitable for large datasets and complex regression problems.\nOperating within the LightGBM framework, the LGBMRegressor leverages the power of ensemble learning, amalgamating the outputs of multiple weak models to formulate a robust collective prediction. The individual weak models in LightGBM are decision trees, which are constructed and refined through a gradient boosting process.\nIn each iteration, the LGBMRegressor constructs decision trees by learning from the residual errors of the preceding iterations, thereby enhancing its predictive capabilities iteratively. However, what sets LightGBM apart is its unique approach to constructing decision trees. It employs a histogram-based algorithm that optimizes the process of binning feature values, enabling faster and more efficient computation. This approach significantly reduces memory consumption and speeds up the training process, contributing to the model\u0026rsquo;s high efficiency and scalability.\nparameters = {\u0026#39;n_estimators\u0026#39;:[600,800], \u0026#39;max_depth\u0026#39;:[8,10], \u0026#39;learning_rate\u0026#39;:[0.04]} model = LGBMRegressor(random_state=44) gs = GridSearchCV(model, parameters, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.04 max_depth 8 n_estimators 600 The evaluation of the model gives the following results:\nevaluate(gs, X_test, y_test) Metric Score Mean Squared Error 232.349011 Root Mean Squared Error 15.242999 Mean Absolute Error 6.929682 R Squared 72.0181% Long Short Term Memory # Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that has proven exceptionally effective in capturing and modeling sequential data, making it a cornerstone of modern machine learning and natural language processing tasks. Unlike traditional feedforward neural networks, LSTMs are explicitly designed to handle sequences by maintaining a memory of past information over extended time intervals, allowing them to comprehend patterns, dependencies, and context within sequential data.\nThe distinguishing feature of LSTMs is their ability to mitigate the vanishing gradient problem commonly encountered in standard RNNs. LSTMs achieve this by incorporating specialized gating mechanisms, including the input gate, forget gate, and output gate. These gates regulate the flow of information within the network, enabling it to retain relevant information over long periods while discarding or updating less relevant data.\nThe LSTM architecture consists of interconnected memory cells that can maintain their states over time, ensuring the network can capture and retain important patterns even across extended sequences. This makes LSTMs particularly well-suited for a wide range of applications, including time series forecasting, speech recognition, sentiment analysis, machine translation, and more. By preserving context and long-term dependencies, LSTMs have become a foundational tool in advancing the capabilities of deep learning models to process and comprehend sequential data with unparalleled accuracy and versatility.\nFor more information about LSTMs, I recommend the following youtube video\nSince LSTMs do not support missing values, our initial task involves addressing this issue.\nX_train.fillna(method=\u0026#39;bfill\u0026#39;, inplace=True) X_train.fillna(method=\u0026#39;ffill\u0026#39;, inplace=True) X_test.fillna(method=\u0026#39;bfill\u0026#39;, inplace=True) X_test.fillna(method=\u0026#39;ffill\u0026#39;, inplace=True) Next we will change the dimensions and data types before feeding the data to our network.\n# Handle the dimensions train_X = np.expand_dims(X_train.values, -1) train_y = np.expand_dims(y_train.values, -1) test_X = np.expand_dims(X_test.values, -1) test_y = np.expand_dims(y_test.values, -1) # Handle the data types train_X = np.array(train_X, dtype=np.float32) train_y = np.array(train_y, dtype=np.float32) test_X = np.array(test_X, dtype=np.float32) test_y = np.array(test_y, dtype=np.float32) Finally, we build our model\nmodel = Sequential() model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]))) model.add(Dense(64, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.2)) model.add(Dense(32, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.2)) model.add(Dense(1)) model.compile(loss=\u0026#39;mse\u0026#39;, optimizer=\u0026#39;adam\u0026#39;) # fit network history = model.fit(train_X, train_y, epochs=50, batch_size=128, validation_data=(test_X, test_y), verbose=2, shuffle=False) # plot history plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;train\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;test\u0026#39;) plt.legend() plt.show() Learning Curve The evaluation of the model gives the following results:\nevaluate(gs, X_test, y_test) Metric Score Mean Squared Error 360.648804 Root Mean Squared Error 18.990755 Mean Absolute Error 11.528387 R Squared 56.5669% Key Findings # Model Performance and Validation: The top-performing model, LGBMRegressor, achieved MAE of 6.92, RMSE of 15.24, and an R2 score of 72.02%. Given the limited 7-day dataset (6 days for training, 1 day for testing), the model slightly underfits due to data scarcity. Data scarcity refers to a situation where the available dataset is limited in size or lacks sufficient diversity. In the context of data science and machine learning, having a small amount of data can lead to challenges in building accurate and reliable models. When data is scarce, there might not be enough examples to capture the underlying patterns, relationships, and variations in the data. This can result in models that are less robust, less accurate, and more prone to overfitting or underfitting.\nData Insights and Patterns: An intriguing discovery emerges from the data: distinct product categories exhibit a dual pricing structure, encompassing both budget-friendly and premium pricing tiers. What\u0026rsquo;s particularly interesting is that the budget-friendly range experiences higher purchase frequency, underscoring the market\u0026rsquo;s responsiveness to different pricing strategies. Lag Choice: We pick lags using a neat trick called partial autocorrelation. It helps the model choose the best past times for predicting the future. The model gets the time connections that make its predictions sharper. Assumptions and Limitations: An important consideration is the model\u0026rsquo;s temporal dependency on present data for hourly forecasts. Consequently, the model\u0026rsquo;s predictive horizon is constrained to a span of one hour. This limitation reflects the model\u0026rsquo;s reliance on immediate historical information to make short-term predictions. Recommendations # Future Steps and Improvements: The path to a more robust forecasting model lies in the acquisition of a larger dataset. A greater volume of data would not only facilitate a deeper understanding of underlying seasonality but also enhance the precision of predictions. By capturing more intricate temporal patterns, the model\u0026rsquo;s accuracy could be substantially improved. Business Implications: A strategic recommendation involves the introduction of a new variable termed \u0026lsquo;restock\u0026rsquo;, which serves as an indicator of stock levels within the grocery shop. By setting a threshold‚Äîfor instance, restocking when stock falls below 50%‚Äîthe problem transforms into a classification task. Comparing the performance of this classification model with the one predicting stock levels can provide valuable insights into the approach used to solve the stock management problem for Gala Groceries. ","date":null,"permalink":"/blogs/artificial-intelligence-virtual-experience-program-cognizant/","section":"Blogs","summary":"Gala Groceries is a technology-led grocery store chain based in the USA. They rely heavily on new technologies, such as IoT to improve their supply chain which gives them a competitive edge over other grocery stores.","title":"Artificial Intelligence Virtual Experience Program - Cognizant"},{"content":"PowerCo is a big company that provides gas and electricity to businesses, medium-sized enterprises, and homes. The energy market in Europe has changed, giving customers more options. This has caused many small and medium-sized businesses to switch to other energy providers, which is a big problem for PowerCo. To figure out why this is happening, they have teamed up with BCG for help.\nObjectives # Gain a comprehensive understanding of the problem at hand and request necessary data from PowerCo to facilitate problem-solving. Conduct a thorough data exploration and examine the hypothesis that price sensitivity is somewhat associated with customer churn. Perform feature engineering and develop a predictive model capable of identifying customers with a high likelihood of churn. Prepare an executive summary summarizing the key findings and recommendations. Understanding the Problem # In business and marketing, churn refers to the rate at which customers or subscribers discontinue their relationship with a company or stop using its products or services. Therefore, the churn problem here refers to the challenge faced by powerCo in retaining its customers and reducing the rate of customer churn.\nUnderstanding customer attrition (churn) is crucial for PowerCo as it allows them to identify patterns, trends, or factors that contribute to customer disengagement. The data needed to determine these patterns is:\nBilling data: Billing data can be used to determine how much customers are paying for their energy usage and how frequently they receive bills. This information can help to identify customers who are likely to be more price sensitive. Usage patterns: Information on customers\u0026rsquo; energy usage patterns, such as the amount of energy they consume, the time of day they consume it, and their peak energy usage, may provide insights into their level of price sensitivity. Interaction history: like contract start date or whether there have been any concerns or complaints expressed by the customers. Any feature that would represent the customer‚Äôs satisfaction level. Churned: whether the customer enterprise churned or not. Another useful data could be competitor‚Äôs information and their pricing so we can compare the costs relative to the competition in the field.\nData Exploration # After assessing the quality of the data and cleaning it. We start our data exploration by visualizing the distribution of customers churn status.\nCustumor Churn Distribution The majority of SME clients who churned have been with the company between 3 to 6 years. A concerning number of 28 SME have churned after being clients for PowerCo for more than 10 years as can be seen in the next barplot:\nCustumor Tenure Distribution The next figure shows that churned SME pay more than those who did not churn. Which suggests that the pricing strategy is the reason of churn.\nPower Price Comparison: Churned Customers vs. Non-Churned Customers Hypothesis Testing: # We will investigate further by hypothesis testing using two methods:\nBootstrap. T-test. We start by stating the null hypothesis and alternative hypothesis:\nNull hypothesis: there is no significant difference between the mean of price of the two groups (churn/retention).\nAlternative hypothesis: there is a significant difference between the mean of price of the two groups (churn/retention).\nThe significance level is assumed to be 5%.\nStatistical significance is a term used in statistical hypothesis testing to describe the likelihood that an observed effect or difference between groups is real and not simply due to chance.\nIf you would like to get technical I highly recommend you checkout the full implementation in this github repository The p-values for both methods (Bootstrap/T-test) for the different price variables are less than the significance level leading to the rejection of the null hypothesis and suggesting that there is a difference between the mean of price of the two groups (churn/retention).\nFeature Engineering # Sum of price for energy and power along with the difference with respect to the period # The changes in the price from one period to another is a good indicator for churn. The expected thing to notice is that SME who churned will have a positive difference meaning that the prices for energy and power are getting high. The inverse is expected for SME who did not churn.\n# price for period 1 price_df[\u0026#39;price_p1\u0026#39;]=price_df[\u0026#39;price_off_peak_var\u0026#39;]+price_df[\u0026#39;price_off_peak_fix\u0026#39;] # price for period 2 price_df[\u0026#39;price_p2\u0026#39;]=price_df[\u0026#39;price_peak_var\u0026#39;]+price_df[\u0026#39;price_peak_fix\u0026#39;] # price for period 3 price_df[\u0026#39;price_p3\u0026#39;]=price_df[\u0026#39;price_mid_peak_var\u0026#39;]+price_df[\u0026#39;price_mid_peak_fix\u0026#39;] # difference between the price for period 2 and period 1 price_df[\u0026#39;pp12\u0026#39;]=price_df[\u0026#39;price_p2\u0026#39;]-price_df[\u0026#39;price_p1\u0026#39;] # difference between the price for period 3 and period 2 price_df[\u0026#39;pp23\u0026#39;]=price_df[\u0026#39;price_p3\u0026#39;]-price_df[\u0026#39;price_p2\u0026#39;] # difference between the price for period 3 and period 1 price_df[\u0026#39;pp13\u0026#39;]=price_df[\u0026#39;price_p3\u0026#39;]-price_df[\u0026#39;price_p1\u0026#39;] Difference between 1st period prices in December and preceding January # This the prices progression in a different and more detailed way.\n# Group off-peak prices by companies and month monthly_price_by_id = price_df.groupby([\u0026#39;id\u0026#39;, \u0026#39;price_date\u0026#39;]).agg({\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;mean\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;mean\u0026#39;}).reset_index() # Get january and december prices jan_prices = monthly_price_by_id.groupby(\u0026#39;id\u0026#39;).first().reset_index() dec_prices = monthly_price_by_id.groupby(\u0026#39;id\u0026#39;).last().reset_index() # Calculate the difference diff_1 = pd.merge(dec_prices.rename(columns={\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;dec_1\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;dec_2\u0026#39;}), jan_prices.rename(columns={\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;jan_1\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;jan_2\u0026#39;}).drop(columns=\u0026#39;price_date\u0026#39;), on=\u0026#39;id\u0026#39;) diff_1[\u0026#39;diff_dec_january_energy_p1\u0026#39;] = diff_1[\u0026#39;dec_1\u0026#39;] - diff_1[\u0026#39;jan_1\u0026#39;] diff_1[\u0026#39;diff_dec_january_power_p1\u0026#39;] = diff_1[\u0026#39;dec_2\u0026#39;] - diff_1[\u0026#39;jan_2\u0026#39;] diff_1 = diff_1[[\u0026#39;id\u0026#39;, \u0026#39;diff_dec_january_energy_p1\u0026#39;,\u0026#39;diff_dec_january_power_p1\u0026#39;]] diff_1.head() Difference between 2nd period/3rd period prices in December and preceding January # These are calculated in the same way as the feature for the first period.\nTenure # Tenure is a very important factor for predicting churn. it is the duration of business between the client and PowerCo.\ndf[\u0026#39;tenure\u0026#39;] = (df[\u0026#39;date_end\u0026#39;]-df[\u0026#39;date_activ\u0026#39;]).dt.days/365 The deviation of last month consumption relative to the mean consumption for one month # This feature aims at detecting any change of pattern in the consumption of the client for the last month.\ndf[\u0026#39;cons_dev\u0026#39;]=(df[\u0026#39;cons_12m\u0026#39;]/12)-df[\u0026#39;cons_last_month\u0026#39;] Ratio of consumption for the next year compared to the current year # This feature shows whether the client is expected to have a larger or lower demand for energy and power in the future compared to his current consumption.\ndf[\u0026#39;cons_pattern\u0026#39;]=df[\u0026#39;forecast_cons_12m\u0026#39;]/df[\u0026#39;cons_12m\u0026#39;] def handleInf(x): if x==float(\u0026#39;-inf\u0026#39;) or x==float(\u0026#39;inf\u0026#39;): return 0 else: return x # handle infinity values df.cons_pattern=df.cons_pattern.apply(handleInf) # handle a few missing values due to a division by 0 df[\u0026#39;cons_pattern\u0026#39;].fillna(0, inplace=True) Lastly, date features will dropped as they are summarized in a more representative feature that is tenure. And we drop any duplicates in the dataset to avoid having the same observations in the training and test sets.\ntrain = pd.merge(price_df.drop([\u0026#39;price_date\u0026#39;, \u0026#39;price_off_peak_var\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;, \u0026#39;price_peak_var\u0026#39;, \u0026#39;price_peak_fix\u0026#39;, \u0026#39;price_mid_peak_var\u0026#39;, \u0026#39;price_mid_peak_fix\u0026#39;], axis=1), df.drop([\u0026#39;date_activ\u0026#39;, \u0026#39;date_end\u0026#39;, \u0026#39;date_modif_prod\u0026#39;, \u0026#39;date_renewal\u0026#39;], axis=1), on=\u0026#39;id\u0026#39;) # Drop duplicates train.drop_duplicates(inplace=True) Predictive Model Development # First, we seperate our target variable from the features that will be used to train the model.\ny = train[\u0026#39;churn\u0026#39;] X = train.drop([\u0026#39;churn\u0026#39;], axis=1).set_index(\u0026#39;id\u0026#39;) Then, we make some preprocessing to the data before giving it to the model.\n# Convert has_gas column datatype to int X[\u0026#39;has_gas\u0026#39;] = X[\u0026#39;has_gas\u0026#39;].astype(int) # Encode channel_sales and origin features using a OneHotEncoder ct = ColumnTransformer(transformers=[(\u0026#39;encoder\u0026#39;, OneHotEncoder(), [6,24])], remainder=\u0026#39;passthrough\u0026#39;) X = np.array(ct.fit_transform(X)) We split the data into training and test sets of sizes 75% and 25% respectively. Making sure that the target variable is similarly distributed in both sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y) The following function will be used to evaluate the model performance. The evaluation metrics chosen are:\naccuracy: however it is not representative for the model performance as predicting all 0\u0026rsquo;s will give an accuracy above 90%. This is why the next evaluation metrics are useed. Precision: it indicates how many of the predicted as churned are actually true. recall: it explains how many of the actual positive cases we were able to detect with our model. f1 score: It gives a combined idea about Precision and Recall metrics. It is maximum when Precision is equal to Recall. ROC AUC: The Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‚Äòsignal‚Äô from the ‚Äònoise‚Äô. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. def evaluate(model_, X_test_, y_test_): y_pred = model_.predict(X_test_) results = pd.DataFrame({\u0026#34;Accuracy\u0026#34; : [accuracy_score(y_test_, y_pred)], \u0026#34;Precision\u0026#34; : [precision_score(y_test_, y_pred)], \u0026#34;Recall\u0026#34; : [recall_score(y_test_, y_pred)], \u0026#34;f1\u0026#34; : [f1_score(y_test_, y_pred)], \u0026#34;ROC AUC\u0026#34; : [roc_auc_score(y_test, y_pred)]}) return results The model used is XGBClassifier: it is an ensemble learning method that combines the predictions of multiple weak models to produce a strong prediction. The weak models in XGBoost are decision trees, which are trained using gradient boosting. This means that at each iteration, the algorithm fits a decision tree to the residuals of the previous iteration.\nOnce the decision trees have been trained, XGBoost makes predictions by combining the predictions of all the trees using a weighted average. The weights for each tree are learned during training using the same objective function. This allows the algorithm to automatically learn which trees are more important and should be given more weight in the final prediction.\nWe used GridSearchCV for hyper parameter tuning.\nparameters = {\u0026#39;n_estimators\u0026#39;:[256,512,1024], \u0026#39;max_depth\u0026#39;:[6,8,12], \u0026#39;learning_rate\u0026#39;:[0.03,0.1]} model = xgb.XGBClassifier(n_jobs=-1, random_state=44, use_label_encoder=False, eval_metric=f1_score) gs = GridSearchCV(model, parameters, scoring=\u0026#39;f1\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.1 max_depth 12 n_estimators 1024 The evaluation of the model gives the following very satisfying results:\nevaluate(gs, X_test, y_test) Metric Score Accuracy 0.997446 Precision 1.0 Recall 0.97397 f1 Score 0.986813 ROC AUC 0.986985 Recommendations # The results obtained on the test set demonstrate outstanding performance in predicting customer churn. The precision metric indicates that all the clients identified as likely to churn did indeed churn, achieving a perfect 100% precision score. This high precision signifies the reliability of the predictive model in accurately identifying potential churners.\nWith such a reliable model in place, PowerCo can confidently implement targeted retention strategies for small and medium-sized enterprises (SMEs) who are predicted to be at high risk of churn. Offering a substantial 20% reduction in their energy costs can serve as a compelling incentive for these SMEs to remain loyal to PowerCo.\nImplementing this personalized approach not only helps retain SME customers but also fosters a sense of loyalty and satisfaction among them. Additionally, it positions PowerCo as a proactive and customer-centric energy provider, capable of anticipating and addressing customer needs effectively.\nIt is important to note that while the model\u0026rsquo;s precision is exceptional, further considerations should be taken into account. Continuously monitoring and refining the predictive model will ensure its accuracy and effectiveness in identifying potential churners. Furthermore, evaluating the impact of the discount offer on customer retention and overall profitability will provide valuable insights for future decision-making and resource allocation.\nOverall, the combination of an accurate predictive model and a targeted discount strategy presents an opportunity for PowerCo to reduce churn rates, enhance customer loyalty, and maintain a competitive edge in the energy market.\nExecutive Summary # Executive Summary ","date":null,"permalink":"/blogs/data-science-and-analytics-virtual-experience-program-bcg/","section":"Blogs","summary":"PowerCo supplies gas and electricity to corporate, SME, and residential customers. The changing energy market in Europe has led many SMEs to switch away from PowerCo, making customer retention challenging.","title":"Data Science \u0026 Analytics Virtual Experience Program - BCG"}]