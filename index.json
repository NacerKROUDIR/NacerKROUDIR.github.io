[{"content":" Data science and artificial intelligence enthusiast with a keen interest in turning raw data into meaningful stories. Proven track record of working with data analytics tools, such as Python, Microsoft Excel, Tableau. Ability to complete tasks on time in both individual and team settings. Dependable and reliable, ready to learn and grow!\nEducation: # Computer Engineering Master\u0026rsquo;s Degree # Institute of Electronics and Electrical Engineering (IEEE Ex: INELEC), University of Boumerdes # September 2021 - July 2023\nRelevant coursework: Data Structures and Algorithms, Probability and Statistics, Database Management System, Computer Networking\nFinal year project:\nEpidemic Simulation Framework: Design, Implementation and Accuracy Analysis: Epidemic Simulation Framework is an open-source project that aims at providing an interactive and educational tool for simulating the spread of a virus in a 2D plane. The project utilizes various parameters such as transmission rate, infection period, and recovery rate to generate different scenarios of an epidemic. The simulation tool is designed to visualize the epidemic in real-time, enabling users to observe the spread of the virus and the impact of various interventions such as quarantine, social distancing, and vaccination. The project aims at educating users about the importance of public health measures and the impact of individual behavior on the spread of an epidemic.\nData Analyst Nanodegree # udacity # November 2022 - February 2023\nA program delivered by Udacity and ALX. With various projects that are focused on different aspects of data analysis, including data wrangling, data visualization, and exploratory data analysis.\nElectronics and Electrical Engineering Bachelor\u0026rsquo;s Degree # Institute of Electronics and Electrical Engineering (IEEE Ex: INELEC), University of Boumerdes # September 2018 / July 2021\nRelevant coursework: Linear Algebra, Calculus, Programming with C, Electrical Engineering, Digital Systems, Active Devices, Computer Architecture\nFinal year project:\nIoT Based Security Camera: A raspberry pi based system that detects faces and track them. Also, it runs face recognition and takes a picture of unrecognized people and sends it via email.\nWork Experience: # Data Analyst Intern # Yassir # July 2022 - September 2022\nInterpreted data and analyzed results using multiple various statistical techniques. Built different classification machine learning models with an accuracy of 97%. Visualized data and communicated the gathered insights. Skills # Skill Level Data Analysis ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Data Visualization ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Machine Learning ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Databases ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Business Intelligence ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Python ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è SQL ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Tableau ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Microsoft Excel ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Organizations: # Co-manager of Communication Department # School of AI Algiers # September 2021 - August 2022\nOrganizer and Active Member in the Developement Department # Inelectronics Student Club # September 2019 - September 2021\nCertificates # Certificate Date Data Scientist with Python - DataCamp April 28, 2022 Fundamentals of Deep Learning - Nvidia December 26, 2021 Databases and SQL for Data Science - IBM December 13,2021 Machine Learning - Coursera September 24, 2021 Injaz El-Djazair - The Entrepreneur\u0026rsquo;s Journey 2019/2020 Languages # Language Level English ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Arabic ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è French ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ","date":"7 July 2023","permalink":"/resume/","section":"Nacer KROUDIR","summary":"Data science and artificial intelligence enthusiast with a keen interest in turning raw data into meaningful stories.","title":"Hi, I'm Nacer KROUDIR üëã"},{"content":"","date":null,"permalink":"/tags/blogs/","section":"Tags","summary":"","title":"blogs"},{"content":" Projects, Articles, Everything that I worked on will be here!\nFor more blogs, you can check out my published blogs on medium.\n","date":null,"permalink":"/blogs/","section":"Blogs","summary":"Projects, Articles, Everything that I worked on will be here!","title":"Blogs"},{"content":"PowerCo is a big company that provides gas and electricity to businesses, medium-sized enterprises, and homes. The energy market in Europe has changed, giving customers more options. This has caused many small and medium-sized businesses to switch to other energy providers, which is a big problem for PowerCo. To figure out why this is happening, they have teamed up with BCG for help.\nObjectives # Gain a comprehensive understanding of the problem at hand and request necessary data from PowerCo to facilitate problem-solving. Conduct a thorough data exploration and examine the hypothesis that price sensitivity is somewhat associated with customer churn. Perform feature engineering and develop a predictive model capable of identifying customers with a high likelihood of churn. Prepare an executive summary summarizing the key findings and recommendations. Understaning the Problem # In business and marketing, churn refers to the rate at which customers or subscribers discontinue their relationship with a company or stop using its products or services. Therefore, the churn problem here refers to the challenge faced by powerCo in retaining its customers and reducing the rate of customer churn.\nUnderstanding customer attrition (churn) is crucial for PowerCo as it allows them to identify patterns, trends, or factors that contribute to customer disengagement. The data needed to determine these patterns is:\nBilling data: Billing data can be used to determine how much customers are paying for their energy usage and how frequently they receive bills. This information can help to identify customers who are likely to be more price sensitive. Usage patterns: Information on customers\u0026rsquo; energy usage patterns, such as the amount of energy they consume, the time of day they consume it, and their peak energy usage, may provide insights into their level of price sensitivity. Interaction history: like contract start date or whether there have been any concerns or complaints expressed by the customers. Any feature that would represent the customer‚Äôs satisfaction level. Churned: whether the customer enterprise churned or not. Another useful data could be competitor‚Äôs information and their pricing so we can compare the costs relative to the competition in the field.\nData Exploration # After assessing the quality of the data and cleaning it. We start our data exploration by visualizing the distribution of customers churn status.\nCustumor Churn Distribution The majority of SME clients who churned have been with the company between 3 to 6 years. A concerning number of 28 SME have churned after being clients for PowerCo for more than 10 years as can be seen in the next barplot:\nCustumor Tenure Distribution The next figure shows that churned SME pay more than those who did not churn. Which suggests that the pricing strategy is the reason of churn.\nPower Price Comparison: Churned Customers vs. Non-Churned Customers Hypothesis Testing: # We will investigate further by hypothesis testing using two methods:\nBootstrap. T-test. We start by stating the null hypothesis and alternative hypothesis:\nNull hypothesis: there is no significant difference between the mean of price of the two groups (churn/retention).\nAlternative hypothesis: there is a significant difference between the mean of price of the two groups (churn/retention).\nThe significance level is assumed to be 5%.\nStatistical significance is a term used in statistical hypothesis testing to describe the likelihood that an observed effect or difference between groups is real and not simply due to chance.\nIf you would like to get technical I highly recommend you checkout the full implementation in this github repository The p-values for both methods (Bootstrap/T-test) for the different price variables are less than the significance level leading to the rejection of the null hypothesis and suggesting that there is a difference between the mean of price of the two groups (churn/retention).\nFeature Engineering # Sum of price for energy and power along with the difference with respect to the period # The changes in the price from one period to another is a good indicator for churn. The expected thing to notice is that SME who churned will have a positive difference meaning that the prices for energy and power are getting high. The inverse is expected for SME who did not churn.\n# price for period 1 price_df[\u0026#39;price_p1\u0026#39;]=price_df[\u0026#39;price_off_peak_var\u0026#39;]+price_df[\u0026#39;price_off_peak_fix\u0026#39;] # price for period 2 price_df[\u0026#39;price_p2\u0026#39;]=price_df[\u0026#39;price_peak_var\u0026#39;]+price_df[\u0026#39;price_peak_fix\u0026#39;] # price for period 3 price_df[\u0026#39;price_p3\u0026#39;]=price_df[\u0026#39;price_mid_peak_var\u0026#39;]+price_df[\u0026#39;price_mid_peak_fix\u0026#39;] # difference between the price for period 2 and period 1 price_df[\u0026#39;pp12\u0026#39;]=price_df[\u0026#39;price_p2\u0026#39;]-price_df[\u0026#39;price_p1\u0026#39;] # difference between the price for period 3 and period 2 price_df[\u0026#39;pp23\u0026#39;]=price_df[\u0026#39;price_p3\u0026#39;]-price_df[\u0026#39;price_p2\u0026#39;] # difference between the price for period 3 and period 1 price_df[\u0026#39;pp13\u0026#39;]=price_df[\u0026#39;price_p3\u0026#39;]-price_df[\u0026#39;price_p1\u0026#39;] Difference between 1st period prices in December and preceding January # This the prices progression in a different and more detailed way.\n# Group off-peak prices by companies and month monthly_price_by_id = price_df.groupby([\u0026#39;id\u0026#39;, \u0026#39;price_date\u0026#39;]).agg({\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;mean\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;mean\u0026#39;}).reset_index() # Get january and december prices jan_prices = monthly_price_by_id.groupby(\u0026#39;id\u0026#39;).first().reset_index() dec_prices = monthly_price_by_id.groupby(\u0026#39;id\u0026#39;).last().reset_index() # Calculate the difference diff_1 = pd.merge(dec_prices.rename(columns={\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;dec_1\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;dec_2\u0026#39;}), jan_prices.rename(columns={\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;jan_1\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;jan_2\u0026#39;}).drop(columns=\u0026#39;price_date\u0026#39;), on=\u0026#39;id\u0026#39;) diff_1[\u0026#39;diff_dec_january_energy_p1\u0026#39;] = diff_1[\u0026#39;dec_1\u0026#39;] - diff_1[\u0026#39;jan_1\u0026#39;] diff_1[\u0026#39;diff_dec_january_power_p1\u0026#39;] = diff_1[\u0026#39;dec_2\u0026#39;] - diff_1[\u0026#39;jan_2\u0026#39;] diff_1 = diff_1[[\u0026#39;id\u0026#39;, \u0026#39;diff_dec_january_energy_p1\u0026#39;,\u0026#39;diff_dec_january_power_p1\u0026#39;]] diff_1.head() Difference between 2nd period/3rd period prices in December and preceding January # These are calculated in the same way as the feature for the first period.\nTenure # Tenure is a very important factor for predicting churn. it is the duration of business between the client and PowerCo.\ndf[\u0026#39;tenure\u0026#39;] = (df[\u0026#39;date_end\u0026#39;]-df[\u0026#39;date_activ\u0026#39;]).dt.days/365 The deviation of last month consumption relative to the mean consumption for one month # This feature aims at detecting any change of pattern in the consumption of the client for the last month.\ndf[\u0026#39;cons_dev\u0026#39;]=(df[\u0026#39;cons_12m\u0026#39;]/12)-df[\u0026#39;cons_last_month\u0026#39;] Ratio of consumption for the next year compared to the current year # This feature shows whether the client is expected to have a larger or lower demand for energy and power in the future compared to his current consumption.\ndf[\u0026#39;cons_pattern\u0026#39;]=df[\u0026#39;forecast_cons_12m\u0026#39;]/df[\u0026#39;cons_12m\u0026#39;] def handleInf(x): if x==float(\u0026#39;-inf\u0026#39;) or x==float(\u0026#39;inf\u0026#39;): return 0 else: return x # handle infinity values df.cons_pattern=df.cons_pattern.apply(handleInf) # handle a few missing values due to a division by 0 df[\u0026#39;cons_pattern\u0026#39;].fillna(0, inplace=True) Lastly, date features will dropped as they are summarized in a more representative feature that is tenure. And we drop any duplicates in the dataset to avoid having the same observations in the training and test sets.\ntrain = pd.merge(price_df.drop([\u0026#39;price_date\u0026#39;, \u0026#39;price_off_peak_var\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;, \u0026#39;price_peak_var\u0026#39;, \u0026#39;price_peak_fix\u0026#39;, \u0026#39;price_mid_peak_var\u0026#39;, \u0026#39;price_mid_peak_fix\u0026#39;], axis=1), df.drop([\u0026#39;date_activ\u0026#39;, \u0026#39;date_end\u0026#39;, \u0026#39;date_modif_prod\u0026#39;, \u0026#39;date_renewal\u0026#39;], axis=1), on=\u0026#39;id\u0026#39;) # Drop duplicates train.drop_duplicates(inplace=True) Predictive Model Development # First, we seperate our target variable from the features that will be used to train the model.\ny = train[\u0026#39;churn\u0026#39;] X = train.drop([\u0026#39;churn\u0026#39;], axis=1).set_index(\u0026#39;id\u0026#39;) Then, we make some preprocessing to the data before giving it to the model.\n# Convert has_gas column datatype to int X[\u0026#39;has_gas\u0026#39;] = X[\u0026#39;has_gas\u0026#39;].astype(int) # Encode channel_sales and origin features using a OneHotEncoder ct = ColumnTransformer(transformers=[(\u0026#39;encoder\u0026#39;, OneHotEncoder(), [6,24])], remainder=\u0026#39;passthrough\u0026#39;) X = np.array(ct.fit_transform(X)) We split the data into training and test sets of sizes 75% and 25% respectively. Making sure that the target variable is similarly distributed in both sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y) The following function will be used to evaluate the model performance. The evaluation metrics chosen are:\naccuracy: however it is not representative for the model performance as predicting all 0\u0026rsquo;s will give an accuracy above 90%. This is why the next evaluation metrics are useed. Precision: it indicates how many of the predicted as churned are actually true. recall: it explains how many of the actual positive cases we were able to detect with our model. f1 score: It gives a combined idea about Precision and Recall metrics. It is maximum when Precision is equal to Recall. ROC AUC: The Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‚Äòsignal‚Äô from the ‚Äònoise‚Äô. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. def evaluate(model_, X_test_, y_test_): y_pred = model_.predict(X_test_) results = pd.DataFrame({\u0026#34;Accuracy\u0026#34; : [accuracy_score(y_test_, y_pred)], \u0026#34;Precision\u0026#34; : [precision_score(y_test_, y_pred)], \u0026#34;Recall\u0026#34; : [recall_score(y_test_, y_pred)], \u0026#34;f1\u0026#34; : [f1_score(y_test_, y_pred)], \u0026#34;ROC AUC\u0026#34; : [roc_auc_score(y_test, y_pred)]}) return results The model used is XGBClassifier: it is an ensemble learning method that combines the predictions of multiple weak models to produce a strong prediction. The weak models in XGBoost are decision trees, which are trained using gradient boosting. This means that at each iteration, the algorithm fits a decision tree to the residuals of the previous iteration.\nOnce the decision trees have been trained, XGBoost makes predictions by combining the predictions of all the trees using a weighted average. The weights for each tree are learned during training using the same objective function. This allows the algorithm to automatically learn which trees are more important and should be given more weight in the final prediction.\nWe used GridSearchCV for hyper parameter tuning.\nparameters = {\u0026#39;n_estimators\u0026#39;:[256,512,1024], \u0026#39;max_depth\u0026#39;:[6,8,12], \u0026#39;learning_rate\u0026#39;:[0.03,0.1]} model = xgb.XGBClassifier(n_jobs=-1, random_state=44, use_label_encoder=False, eval_metric=f1_score) gs = GridSearchCV(model, parameters, scoring=\u0026#39;f1\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.1 max_depth 12 n_estimators 1024 The evaluation of the model gives the following very satisfying results:\nevaluate(gs, X_test, y_test) Metric Score Accuracy 0.997446 Precision 1.0 Recall 0.97397 f1 Score 0.986813 ROC AUC 0.986985 Recommendations # The results obtained on the test set demonstrate outstanding performance in predicting customer churn. The precision metric indicates that all the clients identified as likely to churn did indeed churn, achieving a perfect 100% precision score. This high precision signifies the reliability of the predictive model in accurately identifying potential churners.\nWith such a reliable model in place, PowerCo can confidently implement targeted retention strategies for small and medium-sized enterprises (SMEs) who are predicted to be at high risk of churn. Offering a substantial 20% reduction in their energy costs can serve as a compelling incentive for these SMEs to remain loyal to PowerCo.\nImplementing this personalized approach not only helps retain SME customers but also fosters a sense of loyalty and satisfaction among them. Additionally, it positions PowerCo as a proactive and customer-centric energy provider, capable of anticipating and addressing customer needs effectively.\nIt is important to note that while the model\u0026rsquo;s precision is exceptional, further considerations should be taken into account. Continuously monitoring and refining the predictive model will ensure its accuracy and effectiveness in identifying potential churners. Furthermore, evaluating the impact of the discount offer on customer retention and overall profitability will provide valuable insights for future decision-making and resource allocation.\nOverall, the combination of an accurate predictive model and a targeted discount strategy presents an opportunity for PowerCo to reduce churn rates, enhance customer loyalty, and maintain a competitive edge in the energy market.\nExecutive Summary # Executive Summary ","date":null,"permalink":"/blogs/data-science-and-analytics-virtual-experience-program-bcg/","section":"Blogs","summary":"PowerCo supplies gas and electricity to corporate, SME, and residential customers. The changing energy market in Europe has led many SMEs to switch away from PowerCo, making customer retention challenging.","title":"Data Science \u0026 Analytics Virtual Experience Program - BCG"},{"content":"","date":null,"permalink":"/","section":"Nacer KROUDIR","summary":"","title":"Nacer KROUDIR"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]