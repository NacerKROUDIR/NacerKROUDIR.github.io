[{"content":" I\u0026rsquo;m Nacer, a data engineer at Orange Innovation and a Master\u0026rsquo;s student in Big Data at the University of Paris 8 Vincennes-Saint-Denis, with a strong foundation in data science and a growing passion for AI. During my experiences at DATATEGY and Yassir, I applied statistical methods and built predictive models to solve real-world challenges. I also completed the Udacity Data Analyst Nanodegree, further honing my skills in Python, SQL, Excel, and data visualization. I\u0026rsquo;m particularly interested in Large Language Models (LLMs) and Agentic AI, and I\u0026rsquo;m excited about their potential to transform how we interact with data and build intelligent systems. My goal is to bridge data engineering and AI to drive innovation and meaningful impact.\nWork Experience: # Data Engineer # Orange Innovation # February 2025 - Present\nDesigned dashboards on Superset for Orange mobile applications product owners across 17 countries (Europe, Asia and Africa), and animated presentations to share analysis and insights. Developed and managed a centralized database integrating data from multiple SQL and NoSQL databases. Contributed to developing a chatbot using generative AI to automatically respond to comments on app stores. Data Scientist # DATATEGY # February 2024 - August 2024\nContributed to customer-oriented projects: translation project for RENAULT using GenAI / forecast of groundwater levels in France for summer 2024. Developed an open-source library called FairMango and integrated it with a SaaS platform to create fair models, ensuring compliance with the EU AI Act. A major advantage for the platform over competitors such as Dataiku. Data Scientist # Yassir # May 2022 - September 2022\nDeveloped an artificial intelligence system to predict travel times between two points, reducing costs of using the GoogleMaps API for Yassir. Simulated and evaluated the model\u0026rsquo;s performance over different areas, resulting in a hybrid solution that leveraged the AI model in data-rich areas and the GoogleMaps API in areas with limited data. Software Engineer # Brandt # February 2021 - August 2021\nBuilt an IoT based security camera using Raspberry-pi. Implemented face detection and tracking on two axis of motion using servo motors. Devoloped face recognition and an alarm system that saves the identity of any intruder. Education: # Master in Big Data # University of Paris 8 - Vincennes Saint-Denis # September 2024 - July 2025\nRelevant coursework: Distributed Algorithms for Big Data, Recent Techniques in Data Mining and Embeddings, Connected Objects and Massive Data, Recent Developments in Machine Learning, Big Data Indexing, Formal Models for Big Data, Data Protection and Security, Design Methodology Seminar, Research and Development.\nMaster of Science in Informatics at Grenoble - Data Science and Artificial Intelligence # Ensimag - Grenoble INP # September 2023 - July 2024\nRelevant coursework: From Basic Machine Learning models to Advanced Kernel Learning, Advanced Machine Learning, Fundamentals of Data Processing and Distributed Knowledge, Cloud Computing, Large Scale Data Management.\nproject:\nData Analysis for Google Cluster Dataset: Conducted a detailed analysis with big data using Spark and built a report using insights and visualisations to communicate the key findings.\nMaster in Computer Engineering # Institute of Electronics and Electrical Engineering (IEEE Ex: INELEC), University of Boumerdes # September 2021 - July 2023\nRelevant coursework: Data Structures and Algorithms, Probability and Statistics, Database Management System, Computer Networking\nproject:\nEpidemic Simulation Framework: Epidemic Simulation Framework is an open-source project that aims at providing an interactive and educational tool for simulating the spread of a virus in a 2D plane. The project utilizes various parameters such as transmission rate, infection period, and recovery rate to generate different scenarios of an epidemic. The simulation tool is designed to visualize the epidemic in real-time, enabling users to observe the spread of the virus and the impact of various interventions such as quarantine, social distancing, and vaccination. The project aims at educating users about the importance of public health measures and the impact of individual behavior on the spread of an epidemic.\nData Analyst Nanodegree # udacity # November 2022 - February 2023\nA program delivered by Udacity and ALX. With various projects that are focused on different aspects of data analysis, including data wrangling, data visualization, and exploratory data analysis.\nBachelor in Electronics and Electrical Engineering # Institute of Electronics and Electrical Engineering (IEEE Ex: INELEC), University of Boumerdes # September 2018 / July 2021\nRelevant coursework: Linear Algebra, Calculus, Programming with C, Electrical Engineering, Digital Systems, Active Devices, Computer Architecture\nproject:\nQR Code based Presence Verification System: QR Code based Presence Verification System was developed to target a problem in the presence verification process used by Inelectronics Students Club. It optimises the procedure by generating and tracking participant’s QR codes for events and workshops, effectively managing attendance and reducing errors. It was utilized by the Club for World Engineering Day and several other workshops and events.\nIoT Based Security Camera: A raspberry pi based system that detects faces and track them. Also, it runs face recognition and takes a picture of unrecognized people and sends it via email.\nSkills # Skill Level Generative AI / LLM ⭐️⭐️⭐️⭐️⭐️ Data Analysis ⭐️⭐️⭐️⭐️⭐️ Data Visualization ⭐️⭐️⭐️⭐️⭐️ Machine Learning ⭐️⭐️⭐️⭐️⭐️ Databases ⭐️⭐️⭐️⭐️ Business Intelligence ⭐️⭐️⭐️⭐️⭐️ Python ⭐️⭐️⭐️⭐️⭐️ SQL ⭐️⭐️⭐️⭐️⭐️ Spark ⭐️⭐️⭐️⭐️ Git ⭐️⭐️⭐️⭐️ Google Cloud ⭐️⭐️⭐️⭐️ Big Query ⭐️⭐️⭐️⭐️ Microsoft Excel ⭐️⭐️⭐️⭐️⭐️ Microsoft PowerPoint ⭐️⭐️⭐️⭐️⭐️ Tableau ⭐️⭐️⭐️⭐️ Superset ⭐️⭐️⭐️⭐️⭐️ Figma ⭐️⭐️⭐️⭐️ Organizations: # Co-manager of Communication Department # School of AI Algiers # September 2021 - August 2022\nOrganizer and Active Member in the Developement Department # Inelectronics Student Club # September 2019 - September 2021\nCertificates # Certificate Date Data Scientist with Python - DataCamp April 28, 2022 Fundamentals of Deep Learning - Nvidia December 26, 2021 Databases and SQL for Data Science - IBM December 13,2021 Machine Learning - Coursera September 24, 2021 Injaz El-Djazair - The Entrepreneur\u0026rsquo;s Journey 2019/2020 Languages # Language Level English ⭐️⭐️⭐️⭐️⭐️ French ⭐️⭐️⭐️⭐️⭐️ Arabic ⭐️⭐️⭐️⭐️⭐️ ","date":"10 May 2025","permalink":"/resume/","section":"Nacer KROUDIR","summary":"I\u0026rsquo;m Nacer, a data engineer at Orange Innovation and a Master\u0026rsquo;s student in Big Data at the University of Paris 8 Vincennes-Saint-Denis, with a strong foundation in data science and a growing passion for AI.","title":"Hi, I'm Nacer KROUDIR 👋"},{"content":"","date":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent"},{"content":"","date":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"In today’s competitive job market, crafting the perfect job application can be a daunting task. To make the process easier, I built a Career Advisor AI Agent—a personalized assistant that evaluates your resume for a specific job description and helps you tailor your application. By leveraging the power of GPT-4 and LangChain, this tool provides a score and highlights positive and negative points of your resume across four key criteria—Technical Skills, Experience, Education, and Soft Skills—along with a global score. But that’s not all! The agent also features an interactive chat interface where you can ask questions, like requesting a customized motivation letter tailored to the job description, and get actionable advice using direct references from your resume.\nThis blog walks you through how I built this project, including the tools and techniques used, with code snippets and insights along the way. Before diving in, check out this quick demo video showcasing the project in action!\nIntroduction # Agentic AI # Agentic AI refers to AI systems that act as intelligent agents, capable of autonomously performing tasks and making decisions on behalf of users. These systems are typically designed to assist in a specific domain by interpreting and responding to input from users, offering personalized solutions, and improving over time through interaction.\nThe growing popularity of Agentic AI can be attributed to several factors:\nAutomation of Repetitive Tasks: AI agents can handle tasks that would otherwise be time-consuming for individuals, such as scheduling, answering emails, or generating tailored content. This level of automation frees up users to focus on more complex and creative work. Advances in Natural Language Processing (NLP): With the rise of sophisticated NLP models, such as OpenAI\u0026rsquo;s GPT series, Agentic AI systems can engage in meaningful conversations, interpret instructions, and offer detailed responses, making them much more interactive and efficient than earlier AI systems. Personalization: By processing large amounts of data and learning from user interactions, these AI systems can offer highly personalized advice and solutions. For example, in the case of career development, an AI agent can evaluate resumes against specific job descriptions and provide tailored recommendations. Integration with Existing Platforms: Agentic AI can be integrated into widely used platforms like Slack, Zoom, or in this case, job boards and recruitment systems. These integrations make it easy for users to adopt AI in their daily routines without needing to switch to new tools. Some common tasks automated by Agentic AI include:\nContent Generation: Writing articles, social media posts, or reports. Customer Support: Chatbots that handle inquiries, provide recommendations, and solve problems in real-time. Data Analysis: Automating the process of cleaning, analyzing, and visualizing data. Personalized Recommendations: Tailoring advice, such as for job applications, product suggestions, or learning paths. Used Technologies # The Career Advisor AI Agent relies on a combination of powerful tools and technologies to deliver seamless functionality and an intuitive user experience. Below is a quick overview of the key components that power this AI-driven tool:\nLangChain: LangChain is a framework designed for developing applications powered by language models (such as GPT-4). It simplifies the process of integrating large language models (LLMs) with other tools and data sources, allowing you to build more sophisticated AI-driven applications. In this project, LangChain is used to orchestrate the interactions between the resume, job description, and the evaluation logic, helping to create a coherent and context-aware AI agent. LangChain\u0026rsquo;s ability to interface with multiple data sources and APIs makes it a key tool for handling complex workflows and decision-making processes.\nOpenAI API (GPT-4): The OpenAI API provides access to some of the most powerful language models available today, including GPT-4. In the Career Advisor AI Agent, GPT-4 is used to process both the resume and job description, evaluating them based on criteria like technical skills, experience, education, and soft skills. Additionally, GPT-4 powers the chat interface, allowing users to interact with the AI agent for personalized advice, feedback, and even generating motivation letters. Its advanced natural language understanding makes it ideal for the complex tasks of matching resumes to job descriptions and responding to user queries in a meaningful way.\nStreamlit: Streamlit is a Python framework for building interactive web applications with minimal effort. It allows developers to create user-friendly interfaces that can be easily deployed and shared. Streamlit has been widely adopted by developers, particularly for quick demos and proof-of-concept (POC) projects, due to its simplicity and fast integration capabilities. In the context of Generative AI (GenAI), Streamlit provides an efficient way to showcase AI models and their capabilities in an intuitive and interactive manner. In this project, Streamlit serves as the front-end for the Career Advisor AI Agent, facilitating features like resume uploads, job description input, displaying evaluation results, and enabling the chat interface. Its ease of use and flexibility make it an ideal choice for rapidly developing AI applications and engaging users in a seamless experience.\nKey Features # Resume Upload and Parsing # ApplyPal allows users to easily upload their resumes in PDF format using Streamlit\u0026rsquo;s file uploader component. Once the resume is uploaded, PyPDFLoader from langchain_community.document_loader is used to extract the text from the PDF. This tool efficiently converts the resume\u0026rsquo;s content into a structured format, making it ready for processing by the AI. By combining Streamlit’s simple file upload interface with PyPDFLoader’s powerful text extraction capabilities, the process is seamless, allowing the AI to analyze the resume and generate a comprehensive evaluation based on its content.\nJob Description Input # ApplyPal simplifies the process of providing job descriptions by using a straightforward text input field. Users can copy and paste job descriptions from any platform, such as LinkedIn, Welcome to The Jungle, or company websites, directly into the input field. The structure of the text does not matter—whether it\u0026rsquo;s a neatly formatted job posting or an unstructured snippet, the AI will effectively process it. This flexibility ensures that users can easily provide the required job description without worrying about formatting or compatibility issues, making the process intuitive and user-friendly.\nEvaluation Metrics # To evaluate the candidate\u0026rsquo;s resume against the job description, our AI Agent uses carefully crafted prompt engineering powered by LangChain. By defining specific criteria, the AI assesses the resume on multiple aspects, including Technical Skills, Experience, Education, Soft Skills, and a calculated Global Score. This approach ensures a comprehensive evaluation tailored to the job description.\nThe evaluation process is designed to provide structured and actionable insights by leveraging a predefined output schema. Below is an example of how the evaluation is configured using LangChain:\nskills_schema = ResponseSchema( name=\u0026#34;skills\u0026#34;, description=\u0026#34;Evaluate how well the candidate\u0026#39;s skills align with the job requirements. \u0026#34; \u0026#34;Output a relevance score between 0 and 10. Provide the matched skills in a Python list \u0026#34; \u0026#34;under \u0026#39;positive\u0026#39; and the missing skills in a Python list under \u0026#39;negative\u0026#39;.\u0026#34; ) experience_schema = ResponseSchema( name=\u0026#34;experience\u0026#34;, description=\u0026#34;Evaluate the relevance of the candidate\u0026#39;s professional experience to the job requirements. \u0026#34; \u0026#34;Output a relevance score between 0 and 10. Provide a description of relevant experience \u0026#34; \u0026#34;under \u0026#39;positive\u0026#39; and a description of missing or inadequate experience under \u0026#39;negative\u0026#39;.\u0026#34; ) education_schema = ResponseSchema( name=\u0026#34;education\u0026#34;, description=\u0026#34;Evaluate how well the candidate\u0026#39;s educational background matches the job requirements. \u0026#34; \u0026#34;Output a relevance score between 0 and 10. Provide a description of matching qualifications \u0026#34; \u0026#34;under \u0026#39;positive\u0026#39; and missing qualifications under \u0026#39;negative\u0026#39;.\u0026#34; ) soft_skills_schema = ResponseSchema( name=\u0026#34;soft_skills\u0026#34;, description=\u0026#34;Evaluate any additional relevant aspects, such as certifications, languages, or cultural fit. \u0026#34; \u0026#34;Output a relevance score between 0 and 10. Provide a description of relevant attributes \u0026#34; \u0026#34;under \u0026#39;positive\u0026#39; and any missing or insufficient aspects under \u0026#39;negative\u0026#39;.\u0026#34; ) global_score_schema = ResponseSchema( name=\u0026#34;global_score\u0026#34;, description=\u0026#34;Calculate a weighted global score between 0 and 10 by applying the following weights: \u0026#34; \u0026#34;Skills (40%), Experience (30%), Education (20%), Soft Skills (10%).\u0026#34; ) title_schema = ResponseSchema( name=\u0026#34;title\u0026#34;, description=\u0026#34;The job title and company name. Format: \u0026#39;job_title at company\u0026#39;.\u0026#34; ) response_schemas = [skills_schema, experience_schema, education_schema, soft_skills_schema, global_score_schema, title_schema, ] output_parser = StructuredOutputParser.from_response_schemas(response_schemas) format_instructions = output_parser.get_format_instructions() By including these schemas in the prompt, the AI is guided to return a JSON (or Python dictionary) containing the evaluation results in a structured format. Here’s how the final prompt is constructed to ensure consistent and reliable output:\ntemplate=( \u0026#34;\u0026#34;\u0026#34;Evaluate the relevance of the following resume to the job description and provide detailed feedback\\n Resume: {resume}\\n\\n Job Description: {job_description}\\n\\n {format_instructions}\\nreturn the json only without additional paragraphs\u0026#34;\u0026#34;\u0026#34; ) prompt = ChatPromptTemplate.from_template(template=template) By using this structured prompt, the AI generates output in a predefined format every time. This makes it easy to extract the information programmatically and present it to the user in a concise, clear, and visually appealing manner. Each evaluation criterion is scored on a scale of 0 to 10, with details about matched and missing elements provided for better clarity and actionable feedback.\nChat Interface # The Career Advisor AI includes an intuitive chat interface powered by GPT-4. This interface allows users to interact directly with the AI agent to ask tailored questions about the job application process. For instance, users can request a custom motivation letter, clarify specific job requirements, or seek personalized career advice based on their resume and the job description.\nTo maintain context and consistency, the chat interface leverages Streamlit\u0026rsquo;s session_state for memory management. This ensures that the user’s previous queries and the AI’s responses are stored and can be accessed throughout the session without loss of information.\nAt the core of the chat functionality lies an initial system prompt that defines the AI\u0026rsquo;s behavior. Below is the system prompt used to initialize the chat:\n{ \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; You are a career advisor for a candidate with the resume delimited by \u0026lt;\u0026lt;\u0026lt;\u0026gt;\u0026gt;\u0026gt; and the job description delimited by ((( ))). Resume: \u0026lt;\u0026lt;\u0026lt; {resume} \u0026gt;\u0026gt;\u0026gt; Job description: ((( {job_description} ))) Answer the questions giving the given information only. If the candidate asks a question that you don\u0026#39;t have an answer to, say that you don\u0026#39;t know the answer. \u0026#34;\u0026#34;\u0026#34; } This system prompt ensures that:\nContextual Relevance: The AI only uses the provided resume and job description to answer questions, keeping its responses accurate and specific. Honesty: If the AI cannot infer an answer from the available information, it explicitly states, \u0026ldquo;I don\u0026rsquo;t know the answer.\u0026rdquo; Professional Guidance: The AI consistently behaves like a career advisor, providing helpful, job-specific recommendations. By combining the power of GPT-4 with a well-structured prompt and Streamlit’s session management, the chat interface offers a seamless and highly interactive user experience. Users can pick up conversations exactly where they left off, making the Career Advisor AI both practical and user-friendly.\nHistory # One of the important features of ApplyPal that were added at the very end is its History Functionality, which ensures a seamless and personalized user experience. This feature automatically saves all relevant data from the user\u0026rsquo;s interactions, including:\nJob Descriptions: Each job description provided is saved alongside its evaluation. Resume Evaluations: The scores and detailed feedback across the evaluation metrics (Technical Skills, Experience, Education, Soft Skills, and Global Score) are stored for future reference. Chat History: Every interaction in the chat interface is logged, allowing users to resume their conversation with the AI exactly where they left off. This feature is built using Streamlit\u0026rsquo;s session_state to handle temporary data storage during a session. It is currently basic and stored temporarily while ApplyPal is running; For persistent storage across sessions, a simple database or file-based approach can be employed. Each session can be tied to a unique identifier (e.g., the job title and date), ensuring that users can easily retrieve specific evaluations and conversations.\nBenefits for Users:\nConvenience: Users can revisit previous evaluations without needing to re-upload their resume or job description. Continuity: Conversations with the AI remain intact, making it easier to pick up discussions without starting over. Personalization: By having a history of their interactions, users can compare evaluations for different jobs and refine their applications accordingly. For those who want to get their hands dirty with code and dive deep into the implementation, you can find the project in this repository. Future and Potential Improvements # While the current version of the Career Advisor AI offers robust features, there’s always room for innovation and expansion. In addition to the database that saves user histories for later access, several exciting ideas and enhancements could take this project to the next level:\nIntegrate additional LLM models # Expanding the project to include open-source, free models that run locally (e.g., Llama or Mistral) would reduce dependency on APIs and lower costs for users. This would also make the tool more accessible to those who prefer self-hosted solutions.\nAdd the Ability to Provide a Job URL # Instead of manually inputting or pasting the job description, users could simply provide a URL to the job posting. A scraper could then extract relevant information directly from platforms like LinkedIn or Welcome to The Jungle, making the process even faster and more seamless.\nJob Opportunity Scraper # A more advanced feature could involve integrating a scraper that searches job platforms based on the user\u0026rsquo;s resume. By analyzing the resume, the AI could recommend opportunities that align with the user\u0026rsquo;s skills, experience, and preferences, directly pulling relevant jobs from platforms like LinkedIn, Welcome to The Jungle, or other job boards.\nThis project is an evolving work, and I believe collaboration is key to unlocking its full potential. If you have ideas, suggestions, or skills that could contribute to this project, I’d love to hear from you! Whether you’re an AI enthusiast, a developer, or a career coach with insights to share, feel free to reach out and join the journey of refining this tool further. Together, we can make it even more impactful for job seekers. You can contact me by Email or DM me on Linkedin. Challenges and Learnings # Every project comes with its own set of challenges, and building the Career Advisor AI was no exception. Here are some of the key obstacles I faced and the lessons I learned along the way:\nOutput Format for the Evaluation AI Agent # One of the biggest challenges was ensuring that the LLM adhered strictly to a single output format. Since the data had to be parsed and visualized, consistency in the response format was critical. This was tackled by leveraging LangChain’s structured output tools and extensive prompt engineering. Although achieving this level of precision required multiple iterations and adjustments, the final solution ensures that the AI always returns a well-structured JSON or Python dictionary, which can be easily processed and displayed.\nRethinking the Need for Retrieval-Augmented Generation (RAG) # Initially, I approached the project as a potential use case for RAG (Retrieval-Augmented Generation). However, I quickly realized that the referenced data sources—namely, the resume and the job description—are concise enough to fit comfortably within GPT-4’s context window. Introducing a RAG mechanism, which involves splitting texts, creating embeddings, and retrieving specific parts, would have added unnecessary complexity without any significant benefits. Instead, I opted for a simpler approach that directly passes the relevant data to GPT-4, resulting in a more efficient and streamlined process.\nDesigning the Frontend for Optimal UI/UX # Creating a user-friendly and intuitive interface was another critical aspect of the project. I relied heavily on feedback and tips from my network to refine the design. A special thanks goes to Youcef Salemi, a UI/UX expert, who provided invaluable insights into improving the user experience, and Haithem Herbadji, a talented software engineer, who shared practical advice on interface design. Their guidance ensured that the platform is not only functional but also visually appealing and easy to navigate.\nConclusion # ApplyPal 🧠 marks a significant achievement in my journey of deepening my experience with Generative AI and exploring the vast potential of Agentic AI. This project is more than just a tool; it’s a demonstration of how AI can be harnessed to simplify and enhance complex processes like job applications. By combining technologies like LangChain, OpenAI’s GPT-4, and Streamlit, I was able to create a functional, intuitive platform that empowers job seekers with actionable insights and personalized guidance.\nThis project also reaffirmed the transformative potential of Agentic AI—AI agents capable of automating tasks, delivering intelligent feedback, and facilitating seamless human interaction. The possibilities are immense, and I’m excited to continue pushing the boundaries of what’s possible with these technologies.\n","date":null,"permalink":"/blogs/applypal-how-to-build-your-own-career-advisor-ai-agent/","section":"Blogs","summary":"This blog explores Agentic AI, highlighting its ability to autonomously perform personalized tasks like resume evaluation, career guidance, and application tailoring using advanced NLP models. It showcases ApplyPal, a Career Advisor AI Agent built with GPT-4, LangChain, and Streamlit","title":"ApplyPal 🧠 - How to Build Your Own Career Advisor AI Agent!"},{"content":"","date":null,"permalink":"/tags/blogs/","section":"Tags","summary":"","title":"blogs"},{"content":" Projects, Articles, Everything that I worked on will be here!\nFor more blogs, you can check out my published blogs on medium.\n","date":null,"permalink":"/blogs/","section":"Blogs","summary":"Projects, Articles, Everything that I worked on will be here!","title":"Blogs"},{"content":"","date":null,"permalink":"/tags/evaluation/","section":"Tags","summary":"","title":"Evaluation"},{"content":"","date":null,"permalink":"/tags/genai/","section":"Tags","summary":"","title":"GenAI"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM"},{"content":"","date":null,"permalink":"/","section":"Nacer KROUDIR","summary":"","title":"Nacer KROUDIR"},{"content":"","date":null,"permalink":"/tags/resume/","section":"Tags","summary":"","title":"Resume"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Welcome to the exciting world of accessible large language models (LLMs)! With the advent of tools like Ollama, deploying and running powerful LLMs locally has never been easier. In this blog, we will go through the steps to get started with Ollama, including installing the software and running the latest LLaMA 3.1 model. We\u0026rsquo;ll then take it a step further by deploying our LLM in a Docker container and using Open WebUI for an interactive web-based interface. This setup not only enhances the functionality and performance of the LLM but also provides a user-friendly environment for seamless interactions.\nollama Download Ollama software and the LLM # Ollama is an open-source project designed to simplify the process of running large language models (LLMs) locally. It supports various models, including LLaMA 3, Phi 3, Mistral, and Gemma 2, and is available for macOS, Linux, and Windows. Ollama\u0026rsquo;s key features include compatibility with the OpenAI Chat Completions API, which allows users to leverage existing OpenAI tooling with locally running models, and support for tool calling, enabling models to perform more complex tasks by interacting with external tools​ [1]. Once we have installed Ollama, we are just a step away from interacting with any powerful supported LLM. We will prompted to run this command on the terminal:\nollama run llama3 But we are going to try the latest version released as of (July 30, 2024) which is LLaMA 3.1. The new command is simply:\nollama run llama3.1 This command will download the LLaMA 3.1 model and initialize it. In no time, we will have a fully operational LLaMA 3.1 model right in our terminal, ready for us to interact with. By default, the 8 billion parameter model is downloaded, which is perfect for most applications. However, if you have the computational power and need more advanced capabilities, you can opt for the larger 70 billion or even the 405 billion parameter models.\nTerminal for interaction with LLaMA 3.1 But that\u0026rsquo;s just the beginning!\nElevate Your Experience with Open WebUI # For a more robust and enjoyable experience, we can deploy our LLM in a Docker container and use the Open WebUI for an interactive web-based interface. Docker provides a powerful and consistent environment for running applications, ensuring that our models run smoothly and efficiently across different systems. It simplifies the setup process by encapsulating all the dependencies and configurations within a container, making it easier to deploy and manage the LLM. Open WebUI, on the other hand, offers a seamless, user-friendly interface akin to ChatGPT. This web-based interface enhances our interaction with the LLM by providing a clean and intuitive chat environment. Features like full Markdown and LaTeX support allow us to format responses richly, making the interaction more engaging and informative. Additionally, Open WebUI supports multi-modal interactions, enabling us to integrate text, pdf files and audio inputs seamlessly. With docker open and having the LLM downloaded locally, we use the following command to deploy Open WebUI with Ollama support on a docker container:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main After executing the command, a new container is created and can be observed in the Docker interface. We can now access our WebUI by navigating to http://localhost:3000 in our browser. This link directs us to the fully functional and interactive web-based interface, allowing us to start utilizing our AI model immediately and even offline! We start by selecting the model on the top left\nSelect a model - WebUI interface And Enjoy the chat! You can upload PDF files, and the model will be capable of answering questions based on the content of those files, providing you with a highly interactive and informative experience. A brief demonstration of the LLaMa 3.1 (8B version) capabilities can be seen in the following:\nDemo 1 - WebUI interface Demo 2 - WebUI interface Reflections on Open-Source LLMs # Open-source large language models (LLMs) are transforming the landscape of artificial intelligence and machine learning. These models, which include popular names like LLaMA 3, Mistral, and others supported by projects such as Ollama, offer a range of advantages that are fueling innovation and democratizing access to advanced AI technology. Let\u0026rsquo;s dive into a discussion about the key benefits of open-source LLMs and how they are shaping the future of AI.\nAccessibility and Democratization: Open-source LLMs make cutting-edge technology accessible to a broader audience. Researchers, developers, and enthusiasts can experiment with and build upon these models without the need for expensive licenses or proprietary software. This democratization fosters a more inclusive AI community where innovation can thrive at all levels. Transparency and Trust: Transparency is a fundamental principle of open-source projects. It allows users to inspect the code, understand the model architectures, and verify the training processes, thereby building trust. This openness ensures that there are no hidden biases or undesirable behaviors embedded within the models. However, it\u0026rsquo;s important to note that the training dataset for some models, such as LLaMA 3.1, is not fully disclosed. The LLaMA 3.1 paper mentions that their dataset is created from various sources containing knowledge up to the end of 2023, but it does not provide detailed information about these sources. This lack of clarity raises questions about the ethical collection of the data - a challenge that all LLM development groups face. While transparency in code and architecture is crucial, transparency in data sources is equally important to fully address trust and ethical considerations in AI development. Collaborative Improvement: The open-source community thrives on collaboration. Contributions from diverse groups of developers lead to rapid advancements and improvements. Bugs are identified and fixed more quickly, and new features and optimizations are continuously added, enhancing the overall performance and utility of the models. Customization and Flexibility: Open-source LLMs offer unparalleled flexibility. Users can customize models to suit their specific needs, whether it\u0026rsquo;s fine-tuning them for particular applications, integrating them with other tools, or even modifying the core architecture. This level of customization is often not possible with proprietary models. Cost Efficiency: Open-source models eliminate licensing fees, making advanced AI more affordable. This is particularly beneficial for startups, educational institutions, and non-profit organizations that might not have the resources to invest in costly AI solutions. I quote from Mark Zuckerberg\u0026rsquo;s letter: \u0026ldquo;Developers can run inference on Llama 3.1 405B on their own infra at roughly 50% the cost of using closed models like GPT-4o, for both user-facing and offline inference tasks.\u0026rdquo; Given these advantages, it\u0026rsquo;s clear that open-source LLMs are not just a technical innovation but also a catalyst for broader societal benefits. They empower individuals and organizations to harness the power of AI in ways that were previously unimaginable. How do you think open-source LLMs will continue to shape the future of AI? What potential challenges and opportunities do you foresee in this rapidly evolving landscape?\n","date":null,"permalink":"/blogs/how-to-run-open-source-llms-with-a-chatgpt-like-web-experience/","section":"Blogs","summary":"Open-source large language models (LLMs) offer significant advantages, including enhanced accessibility, transparency, and customization. In this blog, we will explore how to install and run Ollama\u0026rsquo;s LLaMA 3.1 model locally, and deploy it using Docker and Open WebUI for a ChatGPT-like interactive experience.","title":"How to Run Open source LLMs with a ChatGPT-Like Web Experience"},{"content":"Gala Groceries, a US-based tech-driven grocery chain, distinguishes itself through advanced technologies like IoT to gain a competitive advantage. Despite their commitment to offering top-notch, locally sourced fresh produce, maintaining consistent quality throughout the year poses challenges. Seeking Cognizant\u0026rsquo;s assistance, they aim to resolve a supply chain dilemma specific to perishable groceries, seeking to optimize stocking strategies and balance between excess storage costs and customer retention.\nObjectives # Take a deep look at the problem by exploring the sample data provided thoroughly. Come up with a strategic plan to solve the problem, outlining the steps we need to follow. Perform feature engineering and develop a predictive model capable of forecasting the storage levels of each product. Make a pipeline for the Data Engineers to train the model on the provided dataset and make predictions along with evaluation. Understanding the Problem # Stock management, also known as inventory management, is a critical aspect of supply chain management that involves planning, organizing, and controlling the acquisition, storage, distribution, and utilization of goods. It encompasses various strategies and techniques aimed at maintaining an optimal balance between supply and demand while minimizing costs and ensuring efficient operations. Effective stock management is essential for businesses to meet customer demands, reduce carrying costs, and improve overall supply chain performance.\nImportance of Stock Management in the Supply Chain:\nMeeting Customer Demand: Proper stock management ensures that products are available when and where customers want them. It prevents stockouts (running out of products) and overstock situations, allowing businesses to fulfill customer orders promptly and maintain high levels of customer satisfaction. Cost Control: Excess inventory ties up capital and incurs storage costs, while inadequate stock levels can lead to production disruptions and lost sales. Effective stock management helps strike a balance, reducing carrying costs, minimizing waste, and optimizing the use of resources. Supply Chain Efficiency: Efficient stock management contributes to the smooth functioning of the entire supply chain. By ensuring the right products are available at the right time, it minimizes disruptions and delays, leading to improved operational efficiency. Risk Mitigation: Fluctuations in demand, supply chain disruptions, and other external factors can impact a business\u0026rsquo;s ability to maintain steady stock levels. Effective stock management involves risk assessment and mitigation strategies to address potential disruptions and uncertainties. Working Capital Optimization: Excessive inventory ties up capital that could be invested in other areas of the business. Well-managed stock levels free up working capital, allowing companies to invest in growth opportunities or reduce debt. Supplier Relationships: Accurate stock management enables better communication with suppliers. Timely and accurate information about inventory levels helps in negotiating better terms, managing lead times, and building stronger relationships with suppliers. Stock Management Techniques:\nJust-In-Time (JIT): JIT is a strategy where inventory is ordered and received only as needed for production or customer orders. It reduces carrying costs but requires precise coordination and reliable suppliers. Safety Stock: This involves maintaining a certain level of extra inventory to buffer against unexpected demand fluctuations or supply chain disruptions. ABC Analysis: Items are categorized into A, B, and C categories based on their value and contribution to sales. A-items (high-value) are closely monitored, while C-items (low-value) may have less stringent control. Economic Order Quantity (EOQ): EOQ calculates the optimal order quantity that minimizes the total cost of ordering and holding inventory. It considers factors like carrying costs, ordering costs, and demand. Cross-Functional Collaboration: Effective stock management involves collaboration between various departments like sales, marketing, production, and logistics to align inventory levels with overall business goals. Demand Forecasting: Accurate demand forecasting helps predict future demand patterns, enabling businesses to adjust stock levels accordingly. Technology and Automation: Inventory management software, barcoding, RFID, and IoT technologies can provide real-time visibility into stock levels and streamline stock tracking and replenishment processes. In our case, we are combining IoT technologies that measures stock levels of different products with demand forecasting in order develop a model that can accurately predict the demand for each product and the best times to restock.\nData Exploration # After assessing the quality of the data. We start our data exploration by visualizing the distribution of number of transactions per hour.\nNumber of Transactions per Hour The number of transactions throughout the day are somewhat evenly distributed with 11am being the most active hour (738 transactions) and 3pm being the least active hour (669 transactions).\nDistribution of Product Categories There are a few key takeaways from the above barplot, we mention:\nFruits and Vegetables are the most sold categories making up 23.56% of the total transactions. This indicates a high demand for fresh products. packaged foods and baked goods also have a significant percentage of transactions, at 6.48% and 5.66% respectively. Canned foods, refrigerated items, and kitchen products have relatively lower percentages but still make up a noticeable portion of transactions. Meat, dairy, and beverages have similar percentages of around 4-5%, indicating that these categories are popular but are not purchased as much as fruits and vegetables. Cleaning products, baking supplies, snacks, and frozen items all have percentages in the range of 3-4%, indicating moderate demand for these categories. Seafood, medicine, baby products, condiments and sauces, personal care, pets, and spices and herbs have percentages below 3%, indicating a relatively lower demand for these categories. Distribution of Unit Price and Total Price The first plot indicates that the distribution of unit price is positively skewed towards the lower end, implying that the majority of sales involve products with lower prices rather than higher ones.\nThis observation aligns with expectations for a typical grocery store, where a larger number of inexpensive products are sold compared to a smaller number of high-priced items.\nSimilarly, the second plot reveals a similar trend for the total price, indicating a positive skewness in its distribution. This implies that a larger proportion of sales transactions involve lower total prices compared to higher total prices.\nJust as with unit prices, this observation is in line with the expectation that a grocery store would have a higher volume of transactions for lower-priced items, while fewer transactions would involve higher-priced items.\nAverage Unit Price and Total Price per Category From the above figure representing the average unit prices and total prices for each category, here are the key takeaways:\nThe unit prices and total prices vary across different categories, indicating differences in pricing for different types of products. Categories such as baby products, beverages, cleaning products, kitchen items, meat, medicine, and seafood have relatively higher unit prices and total prices. This suggests that these categories include higher-priced or premium products. Categories like fruit, snacks, spices and herbs, and vegetables have lower unit prices and total prices. This indicates that these categories consist of lower-priced or more affordable items. Categories like baked goods, dairy, frozen items, personal care, pets, and condiments and sauces have moderate unit prices and total prices, falling between the higher-priced and lower-priced categories. Distribution of Unit Price per Category The analysis of the unit price distributions reveals an interesting pattern for several categories, namely fruit, vegetables, spices and herbs, canned foods, packaged foods, condiments and sauces, seafood, baking, kitchen, and cleaning products. These categories exhibit bimodal distributions, with two distinct peaks observed at different ends of the price spectrum.\nThis bimodality suggests the presence of two distinct qualities or tiers within these product categories. On one end of the spectrum, there are products with lower unit prices, indicating a more affordable or budget-friendly option. On the other end, there are products with higher unit prices, reflecting a premium or higher-quality offering.\nStrategic Plan # Strategic Plan Feature Engineering # Since we have three datasets – sales, stock level, and temperature – we should combine them using the datetime column. However, prior to merging, it\u0026rsquo;s necessary to round the time values to the nearest hour. Merging datasets can create gaps due to missing values. Our solution involves judiciously applying suitable techniques to fill these gaps, ensuring a continuous and complete dataset. This process transforms missing values into valuable data points. While our dataset initially spans from 9am to 7pm, we recognize the need to encompass the entire day. By skillfully extending the dataset to cover all hours, we capture a broader context for analysis, thereby enriching the predictive capabilities. The dimension of time is unwrapped further, revealing three key temporal features: hour, day, and month. These features introduce depth to the dataset, contributing to a more nuanced and accurate predictive model. The path to accurate stock level prediction is illuminated by the Partial Autocorrelation Function (PACF) plot. Our analysis of this plot guides us to utilize lagged values of 1, 2, and 3, enabling the model to capture meaningful patterns and dependencies. Next we shifted other features -like quantity, temperature and total by one hour as it will be available at the time of making a prediction. customer_type and category were one hot encoded in order to be used for the model development. If you would like to get technical I highly recommend you checkout the full code implementation in this github repository Forecasting Model # First, we split the data by using the first 6 days for training and 1 last day for testing.\ntrain = data[data[\u0026#39;ds\u0026#39;]\u0026lt;\u0026#39;2022-03-07\u0026#39;] test = data[data[\u0026#39;ds\u0026#39;]\u0026gt;=\u0026#39;2022-03-07\u0026#39;] train.drop(\u0026#39;ds\u0026#39;, axis=1, inplace=True) test.drop(\u0026#39;ds\u0026#39;, axis=1, inplace=True) Next, we seperate our target variable from the features that will be used to train the model.\ny_train, y_test = train[\u0026#39;y\u0026#39;], test[\u0026#39;y\u0026#39;] X_train, X_test = train.drop(\u0026#39;y\u0026#39;, axis=1), test.drop(\u0026#39;y\u0026#39;, axis=1) The following function will be used to evaluate the model performance. The evaluation metrics chosen are:\nMean Squared Error: The average of the squared differences between predicted and actual values, used to measure the overall quality of a predictive model. Root Mean Squared Error: The square root of the average of the squared differences between predicted and actual values, providing a measure of the model\u0026rsquo;s prediction error in the same units as the original data. Mean Absolute Error: The average of the absolute differences between predicted and actual values, offering a straightforward measure of the model\u0026rsquo;s prediction accuracy. R Squared: A statistical measure indicating the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. def evaluate(model_, X_test_, y_test_): y_pred = model_.predict(X_test_) results = pd.DataFrame({\u0026#34;MSE\u0026#34; : [mean_squared_error(y_test_, y_pred)], \u0026#34;RMSE\u0026#34; : [np.sqrt(mean_squared_error(y_test_, y_pred))], \u0026#34;MAE\u0026#34; : [mean_absolute_error(y_test_, y_pred)], \u0026#34;R2\u0026#34; : [r2_score(y_test_, y_pred)]}) return results Several machine learning and deep learning algorithms were implemented - namely:\nXGBRegressor LGBMRegressor Long Short Term Memory GridSearchCV was used with XGBRegressor and LGBMRegressor for hyper parameter tuning.\nXGBRegressor # The XGBRegressor is a machine learning model belonging to the XGBoost (Extreme Gradient Boosting) framework: it is an ensemble learning technique designed for regression tasks, proficient at generating robust predictions. Within XGBoost, the XGBRegressor harnesses the collective power of multiple individual weak models to yield a potent predictive model. These weak models, in the context of XGBoost, are decision trees. The model employs a gradient boosting approach, wherein the algorithm iteratively constructs decision trees to minimize the discrepancies between predicted and actual target values.\nOnce the decision trees have been trained, XGBoost makes predictions by combining the predictions of all the trees using a weighted average. The weights for each tree are learned during training using the same objective function. This allows the algorithm to automatically learn which trees are more important and should be given more weight in the final prediction.\nparameters = {\u0026#39;n_estimators\u0026#39;:[128,200], \u0026#39;max_depth\u0026#39;:[7,9], \u0026#39;learning_rate\u0026#39;:[0.03,0.04]} model = xgb.XGBRegressor(n_jobs=-1, random_state=44) gs = GridSearchCV(model, parameters, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.03 max_depth 7 n_estimators 128 The evaluation of the model gives the following results:\nevaluate(gs, X_test, y_test) Metric Score Mean Squared Error 226.496116 Root Mean Squared Error 15.049788 Mean Absolute Error 7.431851 R Squared 72.723% LGBMRegressor # The LGBMRegressor, short for Light Gradient Boosting Machine Regressor, is a sophisticated machine learning model tailored for regression tasks. It falls under the domain of gradient boosting algorithms, specifically designed to provide rapid and highly accurate predictions. The distinguishing feature of the LGBMRegressor lies in its emphasis on optimization for efficiency and performance, making it particularly suitable for large datasets and complex regression problems.\nOperating within the LightGBM framework, the LGBMRegressor leverages the power of ensemble learning, amalgamating the outputs of multiple weak models to formulate a robust collective prediction. The individual weak models in LightGBM are decision trees, which are constructed and refined through a gradient boosting process.\nIn each iteration, the LGBMRegressor constructs decision trees by learning from the residual errors of the preceding iterations, thereby enhancing its predictive capabilities iteratively. However, what sets LightGBM apart is its unique approach to constructing decision trees. It employs a histogram-based algorithm that optimizes the process of binning feature values, enabling faster and more efficient computation. This approach significantly reduces memory consumption and speeds up the training process, contributing to the model\u0026rsquo;s high efficiency and scalability.\nparameters = {\u0026#39;n_estimators\u0026#39;:[600,800], \u0026#39;max_depth\u0026#39;:[8,10], \u0026#39;learning_rate\u0026#39;:[0.04]} model = LGBMRegressor(random_state=44) gs = GridSearchCV(model, parameters, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.04 max_depth 8 n_estimators 600 The evaluation of the model gives the following results:\nevaluate(gs, X_test, y_test) Metric Score Mean Squared Error 232.349011 Root Mean Squared Error 15.242999 Mean Absolute Error 6.929682 R Squared 72.0181% Long Short Term Memory # Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that has proven exceptionally effective in capturing and modeling sequential data, making it a cornerstone of modern machine learning and natural language processing tasks. Unlike traditional feedforward neural networks, LSTMs are explicitly designed to handle sequences by maintaining a memory of past information over extended time intervals, allowing them to comprehend patterns, dependencies, and context within sequential data.\nThe distinguishing feature of LSTMs is their ability to mitigate the vanishing gradient problem commonly encountered in standard RNNs. LSTMs achieve this by incorporating specialized gating mechanisms, including the input gate, forget gate, and output gate. These gates regulate the flow of information within the network, enabling it to retain relevant information over long periods while discarding or updating less relevant data.\nThe LSTM architecture consists of interconnected memory cells that can maintain their states over time, ensuring the network can capture and retain important patterns even across extended sequences. This makes LSTMs particularly well-suited for a wide range of applications, including time series forecasting, speech recognition, sentiment analysis, machine translation, and more. By preserving context and long-term dependencies, LSTMs have become a foundational tool in advancing the capabilities of deep learning models to process and comprehend sequential data with unparalleled accuracy and versatility.\nFor more information about LSTMs, I recommend the following youtube video\nSince LSTMs do not support missing values, our initial task involves addressing this issue.\nX_train.fillna(method=\u0026#39;bfill\u0026#39;, inplace=True) X_train.fillna(method=\u0026#39;ffill\u0026#39;, inplace=True) X_test.fillna(method=\u0026#39;bfill\u0026#39;, inplace=True) X_test.fillna(method=\u0026#39;ffill\u0026#39;, inplace=True) Next we will change the dimensions and data types before feeding the data to our network.\n# Handle the dimensions train_X = np.expand_dims(X_train.values, -1) train_y = np.expand_dims(y_train.values, -1) test_X = np.expand_dims(X_test.values, -1) test_y = np.expand_dims(y_test.values, -1) # Handle the data types train_X = np.array(train_X, dtype=np.float32) train_y = np.array(train_y, dtype=np.float32) test_X = np.array(test_X, dtype=np.float32) test_y = np.array(test_y, dtype=np.float32) Finally, we build our model\nmodel = Sequential() model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]))) model.add(Dense(64, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.2)) model.add(Dense(32, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.2)) model.add(Dense(1)) model.compile(loss=\u0026#39;mse\u0026#39;, optimizer=\u0026#39;adam\u0026#39;) # fit network history = model.fit(train_X, train_y, epochs=50, batch_size=128, validation_data=(test_X, test_y), verbose=2, shuffle=False) # plot history plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;train\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;test\u0026#39;) plt.legend() plt.show() Learning Curve The evaluation of the model gives the following results:\nevaluate(gs, X_test, y_test) Metric Score Mean Squared Error 360.648804 Root Mean Squared Error 18.990755 Mean Absolute Error 11.528387 R Squared 56.5669% Key Findings # Model Performance and Validation: The top-performing model, LGBMRegressor, achieved MAE of 6.92, RMSE of 15.24, and an R2 score of 72.02%. Given the limited 7-day dataset (6 days for training, 1 day for testing), the model slightly underfits due to data scarcity. Data scarcity refers to a situation where the available dataset is limited in size or lacks sufficient diversity. In the context of data science and machine learning, having a small amount of data can lead to challenges in building accurate and reliable models. When data is scarce, there might not be enough examples to capture the underlying patterns, relationships, and variations in the data. This can result in models that are less robust, less accurate, and more prone to overfitting or underfitting.\nData Insights and Patterns: An intriguing discovery emerges from the data: distinct product categories exhibit a dual pricing structure, encompassing both budget-friendly and premium pricing tiers. What\u0026rsquo;s particularly interesting is that the budget-friendly range experiences higher purchase frequency, underscoring the market\u0026rsquo;s responsiveness to different pricing strategies. Lag Choice: We pick lags using a neat trick called partial autocorrelation. It helps the model choose the best past times for predicting the future. The model gets the time connections that make its predictions sharper. Assumptions and Limitations: An important consideration is the model\u0026rsquo;s temporal dependency on present data for hourly forecasts. Consequently, the model\u0026rsquo;s predictive horizon is constrained to a span of one hour. This limitation reflects the model\u0026rsquo;s reliance on immediate historical information to make short-term predictions. Recommendations # Future Steps and Improvements: The path to a more robust forecasting model lies in the acquisition of a larger dataset. A greater volume of data would not only facilitate a deeper understanding of underlying seasonality but also enhance the precision of predictions. By capturing more intricate temporal patterns, the model\u0026rsquo;s accuracy could be substantially improved. Business Implications: A strategic recommendation involves the introduction of a new variable termed \u0026lsquo;restock\u0026rsquo;, which serves as an indicator of stock levels within the grocery shop. By setting a threshold—for instance, restocking when stock falls below 50%—the problem transforms into a classification task. Comparing the performance of this classification model with the one predicting stock levels can provide valuable insights into the approach used to solve the stock management problem for Gala Groceries. ","date":null,"permalink":"/blogs/artificial-intelligence-virtual-experience-program-cognizant/","section":"Blogs","summary":"Gala Groceries is a technology-led grocery store chain based in the USA. They rely heavily on new technologies, such as IoT to improve their supply chain which gives them a competitive edge over other grocery stores.","title":"Artificial Intelligence Virtual Experience Program - Cognizant"},{"content":"PowerCo is a big company that provides gas and electricity to businesses, medium-sized enterprises, and homes. The energy market in Europe has changed, giving customers more options. This has caused many small and medium-sized businesses to switch to other energy providers, which is a big problem for PowerCo. To figure out why this is happening, they have teamed up with BCG for help.\nObjectives # Gain a comprehensive understanding of the problem at hand and request necessary data from PowerCo to facilitate problem-solving. Conduct a thorough data exploration and examine the hypothesis that price sensitivity is somewhat associated with customer churn. Perform feature engineering and develop a predictive model capable of identifying customers with a high likelihood of churn. Prepare an executive summary summarizing the key findings and recommendations. Understanding the Problem # In business and marketing, churn refers to the rate at which customers or subscribers discontinue their relationship with a company or stop using its products or services. Therefore, the churn problem here refers to the challenge faced by powerCo in retaining its customers and reducing the rate of customer churn.\nUnderstanding customer attrition (churn) is crucial for PowerCo as it allows them to identify patterns, trends, or factors that contribute to customer disengagement. The data needed to determine these patterns is:\nBilling data: Billing data can be used to determine how much customers are paying for their energy usage and how frequently they receive bills. This information can help to identify customers who are likely to be more price sensitive. Usage patterns: Information on customers\u0026rsquo; energy usage patterns, such as the amount of energy they consume, the time of day they consume it, and their peak energy usage, may provide insights into their level of price sensitivity. Interaction history: like contract start date or whether there have been any concerns or complaints expressed by the customers. Any feature that would represent the customer’s satisfaction level. Churned: whether the customer enterprise churned or not. Another useful data could be competitor’s information and their pricing so we can compare the costs relative to the competition in the field.\nData Exploration # After assessing the quality of the data and cleaning it. We start our data exploration by visualizing the distribution of customers churn status.\nCustumor Churn Distribution The majority of SME clients who churned have been with the company between 3 to 6 years. A concerning number of 28 SME have churned after being clients for PowerCo for more than 10 years as can be seen in the next barplot:\nCustumor Tenure Distribution The next figure shows that churned SME pay more than those who did not churn. Which suggests that the pricing strategy is the reason of churn.\nPower Price Comparison: Churned Customers vs. Non-Churned Customers Hypothesis Testing: # We will investigate further by hypothesis testing using two methods:\nBootstrap. T-test. We start by stating the null hypothesis and alternative hypothesis:\nNull hypothesis: there is no significant difference between the mean of price of the two groups (churn/retention).\nAlternative hypothesis: there is a significant difference between the mean of price of the two groups (churn/retention).\nThe significance level is assumed to be 5%.\nStatistical significance is a term used in statistical hypothesis testing to describe the likelihood that an observed effect or difference between groups is real and not simply due to chance.\nIf you would like to get technical I highly recommend you checkout the full implementation in this github repository The p-values for both methods (Bootstrap/T-test) for the different price variables are less than the significance level leading to the rejection of the null hypothesis and suggesting that there is a difference between the mean of price of the two groups (churn/retention).\nFeature Engineering # Sum of price for energy and power along with the difference with respect to the period # The changes in the price from one period to another is a good indicator for churn. The expected thing to notice is that SME who churned will have a positive difference meaning that the prices for energy and power are getting high. The inverse is expected for SME who did not churn.\n# price for period 1 price_df[\u0026#39;price_p1\u0026#39;]=price_df[\u0026#39;price_off_peak_var\u0026#39;]+price_df[\u0026#39;price_off_peak_fix\u0026#39;] # price for period 2 price_df[\u0026#39;price_p2\u0026#39;]=price_df[\u0026#39;price_peak_var\u0026#39;]+price_df[\u0026#39;price_peak_fix\u0026#39;] # price for period 3 price_df[\u0026#39;price_p3\u0026#39;]=price_df[\u0026#39;price_mid_peak_var\u0026#39;]+price_df[\u0026#39;price_mid_peak_fix\u0026#39;] # difference between the price for period 2 and period 1 price_df[\u0026#39;pp12\u0026#39;]=price_df[\u0026#39;price_p2\u0026#39;]-price_df[\u0026#39;price_p1\u0026#39;] # difference between the price for period 3 and period 2 price_df[\u0026#39;pp23\u0026#39;]=price_df[\u0026#39;price_p3\u0026#39;]-price_df[\u0026#39;price_p2\u0026#39;] # difference between the price for period 3 and period 1 price_df[\u0026#39;pp13\u0026#39;]=price_df[\u0026#39;price_p3\u0026#39;]-price_df[\u0026#39;price_p1\u0026#39;] Difference between 1st period prices in December and preceding January # This the prices progression in a different and more detailed way.\n# Group off-peak prices by companies and month monthly_price_by_id = price_df.groupby([\u0026#39;id\u0026#39;, \u0026#39;price_date\u0026#39;]).agg({\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;mean\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;mean\u0026#39;}).reset_index() # Get january and december prices jan_prices = monthly_price_by_id.groupby(\u0026#39;id\u0026#39;).first().reset_index() dec_prices = monthly_price_by_id.groupby(\u0026#39;id\u0026#39;).last().reset_index() # Calculate the difference diff_1 = pd.merge(dec_prices.rename(columns={\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;dec_1\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;dec_2\u0026#39;}), jan_prices.rename(columns={\u0026#39;price_off_peak_var\u0026#39;: \u0026#39;jan_1\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;: \u0026#39;jan_2\u0026#39;}).drop(columns=\u0026#39;price_date\u0026#39;), on=\u0026#39;id\u0026#39;) diff_1[\u0026#39;diff_dec_january_energy_p1\u0026#39;] = diff_1[\u0026#39;dec_1\u0026#39;] - diff_1[\u0026#39;jan_1\u0026#39;] diff_1[\u0026#39;diff_dec_january_power_p1\u0026#39;] = diff_1[\u0026#39;dec_2\u0026#39;] - diff_1[\u0026#39;jan_2\u0026#39;] diff_1 = diff_1[[\u0026#39;id\u0026#39;, \u0026#39;diff_dec_january_energy_p1\u0026#39;,\u0026#39;diff_dec_january_power_p1\u0026#39;]] diff_1.head() Difference between 2nd period/3rd period prices in December and preceding January # These are calculated in the same way as the feature for the first period.\nTenure # Tenure is a very important factor for predicting churn. it is the duration of business between the client and PowerCo.\ndf[\u0026#39;tenure\u0026#39;] = (df[\u0026#39;date_end\u0026#39;]-df[\u0026#39;date_activ\u0026#39;]).dt.days/365 The deviation of last month consumption relative to the mean consumption for one month # This feature aims at detecting any change of pattern in the consumption of the client for the last month.\ndf[\u0026#39;cons_dev\u0026#39;]=(df[\u0026#39;cons_12m\u0026#39;]/12)-df[\u0026#39;cons_last_month\u0026#39;] Ratio of consumption for the next year compared to the current year # This feature shows whether the client is expected to have a larger or lower demand for energy and power in the future compared to his current consumption.\ndf[\u0026#39;cons_pattern\u0026#39;]=df[\u0026#39;forecast_cons_12m\u0026#39;]/df[\u0026#39;cons_12m\u0026#39;] def handleInf(x): if x==float(\u0026#39;-inf\u0026#39;) or x==float(\u0026#39;inf\u0026#39;): return 0 else: return x # handle infinity values df.cons_pattern=df.cons_pattern.apply(handleInf) # handle a few missing values due to a division by 0 df[\u0026#39;cons_pattern\u0026#39;].fillna(0, inplace=True) Lastly, date features will dropped as they are summarized in a more representative feature that is tenure. And we drop any duplicates in the dataset to avoid having the same observations in the training and test sets.\ntrain = pd.merge(price_df.drop([\u0026#39;price_date\u0026#39;, \u0026#39;price_off_peak_var\u0026#39;, \u0026#39;price_off_peak_fix\u0026#39;, \u0026#39;price_peak_var\u0026#39;, \u0026#39;price_peak_fix\u0026#39;, \u0026#39;price_mid_peak_var\u0026#39;, \u0026#39;price_mid_peak_fix\u0026#39;], axis=1), df.drop([\u0026#39;date_activ\u0026#39;, \u0026#39;date_end\u0026#39;, \u0026#39;date_modif_prod\u0026#39;, \u0026#39;date_renewal\u0026#39;], axis=1), on=\u0026#39;id\u0026#39;) # Drop duplicates train.drop_duplicates(inplace=True) Predictive Model Development # First, we seperate our target variable from the features that will be used to train the model.\ny = train[\u0026#39;churn\u0026#39;] X = train.drop([\u0026#39;churn\u0026#39;], axis=1).set_index(\u0026#39;id\u0026#39;) Then, we make some preprocessing to the data before giving it to the model.\n# Convert has_gas column datatype to int X[\u0026#39;has_gas\u0026#39;] = X[\u0026#39;has_gas\u0026#39;].astype(int) # Encode channel_sales and origin features using a OneHotEncoder ct = ColumnTransformer(transformers=[(\u0026#39;encoder\u0026#39;, OneHotEncoder(), [6,24])], remainder=\u0026#39;passthrough\u0026#39;) X = np.array(ct.fit_transform(X)) We split the data into training and test sets of sizes 75% and 25% respectively. Making sure that the target variable is similarly distributed in both sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y) The following function will be used to evaluate the model performance. The evaluation metrics chosen are:\naccuracy: however it is not representative for the model performance as predicting all 0\u0026rsquo;s will give an accuracy above 90%. This is why the next evaluation metrics are useed. Precision: it indicates how many of the predicted as churned are actually true. recall: it explains how many of the actual positive cases we were able to detect with our model. f1 score: It gives a combined idea about Precision and Recall metrics. It is maximum when Precision is equal to Recall. ROC AUC: The Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. def evaluate(model_, X_test_, y_test_): y_pred = model_.predict(X_test_) results = pd.DataFrame({\u0026#34;Accuracy\u0026#34; : [accuracy_score(y_test_, y_pred)], \u0026#34;Precision\u0026#34; : [precision_score(y_test_, y_pred)], \u0026#34;Recall\u0026#34; : [recall_score(y_test_, y_pred)], \u0026#34;f1\u0026#34; : [f1_score(y_test_, y_pred)], \u0026#34;ROC AUC\u0026#34; : [roc_auc_score(y_test, y_pred)]}) return results The model used is XGBClassifier: it is an ensemble learning method that combines the predictions of multiple weak models to produce a strong prediction. The weak models in XGBoost are decision trees, which are trained using gradient boosting. This means that at each iteration, the algorithm fits a decision tree to the residuals of the previous iteration.\nOnce the decision trees have been trained, XGBoost makes predictions by combining the predictions of all the trees using a weighted average. The weights for each tree are learned during training using the same objective function. This allows the algorithm to automatically learn which trees are more important and should be given more weight in the final prediction.\nWe used GridSearchCV for hyper parameter tuning.\nparameters = {\u0026#39;n_estimators\u0026#39;:[256,512,1024], \u0026#39;max_depth\u0026#39;:[6,8,12], \u0026#39;learning_rate\u0026#39;:[0.03,0.1]} model = xgb.XGBClassifier(n_jobs=-1, random_state=44, use_label_encoder=False, eval_metric=f1_score) gs = GridSearchCV(model, parameters, scoring=\u0026#39;f1\u0026#39;, cv=5) gs.fit(X_train, y_train) The results shows that the best parameters for our model are summarized in the next table:\nParameter Value learning_rate 0.1 max_depth 12 n_estimators 1024 The evaluation of the model gives the following very satisfying results:\nevaluate(gs, X_test, y_test) Metric Score Accuracy 0.997446 Precision 1.0 Recall 0.97397 f1 Score 0.986813 ROC AUC 0.986985 Recommendations # The results obtained on the test set demonstrate outstanding performance in predicting customer churn. The precision metric indicates that all the clients identified as likely to churn did indeed churn, achieving a perfect 100% precision score. This high precision signifies the reliability of the predictive model in accurately identifying potential churners.\nWith such a reliable model in place, PowerCo can confidently implement targeted retention strategies for small and medium-sized enterprises (SMEs) who are predicted to be at high risk of churn. Offering a substantial 20% reduction in their energy costs can serve as a compelling incentive for these SMEs to remain loyal to PowerCo.\nImplementing this personalized approach not only helps retain SME customers but also fosters a sense of loyalty and satisfaction among them. Additionally, it positions PowerCo as a proactive and customer-centric energy provider, capable of anticipating and addressing customer needs effectively.\nIt is important to note that while the model\u0026rsquo;s precision is exceptional, further considerations should be taken into account. Continuously monitoring and refining the predictive model will ensure its accuracy and effectiveness in identifying potential churners. Furthermore, evaluating the impact of the discount offer on customer retention and overall profitability will provide valuable insights for future decision-making and resource allocation.\nOverall, the combination of an accurate predictive model and a targeted discount strategy presents an opportunity for PowerCo to reduce churn rates, enhance customer loyalty, and maintain a competitive edge in the energy market.\nExecutive Summary # Executive Summary ","date":null,"permalink":"/blogs/data-science-and-analytics-virtual-experience-program-bcg/","section":"Blogs","summary":"PowerCo supplies gas and electricity to corporate, SME, and residential customers. The changing energy market in Europe has led many SMEs to switch away from PowerCo, making customer retention challenging.","title":"Data Science \u0026 Analytics Virtual Experience Program - BCG"}]